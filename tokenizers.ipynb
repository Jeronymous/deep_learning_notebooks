{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔎 Tokenizers (in Deep Learning)\n",
    "\n",
    "## 📖 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer maps `string` $\\rightleftharpoons$ `list of tokens`.\n",
    "* `encode`(\"string\") $\\mapsto$ [\"list\", \"of\", \"tokens\"]\n",
    "* `decode`([\"list\", \"of\", \"tokens\"]) $\\mapsto$ \"string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DeepLearning/NLP, a token is a unit of text (sequence of characters or bytes) that is often characterized by:\n",
    "* an <u>index in the vocabulary</u> of tokens,\n",
    "* a string representer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index is used to interpret inputs and/or outputs of a neural network.\n",
    "<br> It is a positive integer that is often used to index vectorial embeddings.\n",
    "<br> The *encoding* part is used when the neural net is fed with text data (ex: Language Models).\n",
    "<br> The *decoding* part is used when the neural net outputs text data (ex: Text Generation with LM, Automatic Speech Recognition, Image Caption Generation, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When extracted from a string, a token can also be characterized the <u>positions in the original string</u> (start & end). <br>\n",
    "This is needed for application like extractive question answering, where we have to track the position of the answer in the original text to extract it. <br>\n",
    "(Caution with multi-bytes characters: position in bytes is not the same as position in characters.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good tokenizer can compress a string into a small number of tokens,\n",
    "while keeping a reasonable vocabulary size (number of possible unique tokens). <br>\n",
    "A popular measure of this efficiency is the <u>fertility</u> of the tokenizer, defined as the average number of tokens per word:\n",
    "$$\\text{fertility} = \\frac{\\text{number of tokens}}{\\text{number of words}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decomposition of a string into tokens can be illustrated in this small code, where tokens can be words/characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CharSplitter ==\n",
      "-> ['M', 'a', 'i', 's', ',', ' ', 'm', 'a', 'i', 's', '…', ' ', 'v', 'a', 's', ' ', 't', \"'\", 'e', 'n', ' ', 'l', 'à', '-', 'b', 'a', 's', ' ', '!']\n",
      "-> Mais, mais… vas t'en là-bas !\n",
      "\n",
      "== WordSplitter ==\n",
      "-> ['Mais,', '▁mais…', '▁vas', \"▁t'en\", '▁là-bas', '▁!']\n",
      "-> Mais, mais… vas t'en là-bas !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class CharSplitter:\n",
    "\n",
    "    def split(self, text: str) -> list:\n",
    "        return list(text)\n",
    "\n",
    "    def join(self, tokens: list) -> str:\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "class WordSplitter:\n",
    "\n",
    "    _SPACE = \"▁\"\n",
    "\n",
    "    def split(self, text: str) -> list:\n",
    "        words = text.split(\" \")\n",
    "        return ([words[0]] + [self._SPACE+w for w in words[1:]]) if words else []\n",
    "        \n",
    "    def join(self, tokens: list) -> str:\n",
    "        return \"\".join(tokens).replace(self._SPACE, \" \")\n",
    "        \n",
    "input = \"Mais, mais… vas t'en là-bas !\"\n",
    "\n",
    "for tokenizer in [\n",
    "    CharSplitter(),\n",
    "    WordSplitter(),\n",
    "    ]:\n",
    "\n",
    "    encoded = tokenizer.split(input)\n",
    "\n",
    "    # Round-trip test\n",
    "    encoded_decoded = tokenizer.join(encoded)\n",
    "    assert encoded_decoded == input\n",
    "    \n",
    "    print(f\"== {tokenizer.__class__.__name__} ==\\n-> {encoded}\\n-> {encoded_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was to show the string decomposition principle.\n",
    "\n",
    "In reality, tokenizers are more complex when they are used with a neural network model, because of the vocabulary that has a **fixed size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    _SOS = \"<start>\"\n",
    "    _EOS = \"<end>\"\n",
    "    _UNK = \"<unk>\"\n",
    "    vocabulary = [_UNK, _SOS, _EOS]\n",
    "   \n",
    "    def encode(self, text: str) -> list:\n",
    "        # Dec\n",
    "        tokens_str = self.split(text)\n",
    "        tokens_idx = [\n",
    "            self.vocabulary.index(t) if t in self.vocabulary\n",
    "            else self.vocabulary.index(self._UNK)\n",
    "            for t in tokens_str\n",
    "        ]\n",
    "        return tokens_idx\n",
    "        \n",
    "    def encode_str(self, text: str) -> list:\n",
    "        return [self.vocabulary[idx] for idx in self.encode(text)]\n",
    "\n",
    "class CharTokenizer(Tokenizer, CharSplitter):\n",
    "\n",
    "    def __init__(self, vocabulary: list = [chr(i) for i in range(128)]):\n",
    "        self.vocabulary += vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mais, mais… vas t'en là-bas !\n",
      "-> [80, 100, 108, 118, 47, 35, 112, 100, 108, 118, 0, 35, 121, 100, 118, 35, 119, 42, 104, 113, 35, 111, 0, 48, 101, 100, 118, 35, 36]\n",
      "-> ['M', 'a', 'i', 's', ',', ' ', 'm', 'a', 'i', 's', '<unk>', ' ', 'v', 'a', 's', ' ', 't', \"'\", 'e', 'n', ' ', 'l', '<unk>', '-', 'b', 'a', 's', ' ', '!']\n",
      "-> Mais, mais<unk> vas t'en l<unk>-bas !\n"
     ]
    }
   ],
   "source": [
    "input = \"Mais, mais… vas t'en là-bas !\"\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "encoded = tokenizer.encode(input)\n",
    "encoded_str = tokenizer.encode_str(input)\n",
    "encoded_decoded = tokenizer.join(encoded_str)\n",
    "print(f\"{input}\\n-> {encoded}\\n-> {encoded_str}\\n-> {encoded_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Install common librairies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular python libraries for tokenizers are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular tokenizer libraries\n",
    "!pip install tokenizers>=0.20,<0.21 # last version is buggy\n",
    "!pip install tiktoken #==0.8.0\n",
    "!pip install sentencepiece #==0.2.0\n",
    "\n",
    "# Librairies for neural networks that include tokenizers (usually wrap other lower-level tokenization libraries)\n",
    "!pip install transformers #==4.46.3\n",
    "# !pip install nemo #==6.0.3\n",
    "!pip install git+https://github.com/linagora-labs/NeMo.git pytorch_lightning==2.4.0 lhotse==1.28.0\n",
    "#!python -m pip install git+https://github.com/linagora-labs/NeMo.git@{main}#egg=nemo_toolkit[asr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip index versions tiktoken 2> /dev/null\n",
    "!pip index versions tokenizers 2> /dev/null\n",
    "!pip index versions sentencepiece 2> /dev/null\n",
    "\n",
    "# !pip index versions transformers 2> /dev/null\n",
    "# !pip index versions nemo 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also useful for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👨‍💻 Helpers for an inspection of several tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of interesting tokenizers, identified either by their tiktoken name or [Hugging Face repository name](https://huggingface.co/docs/huggingface_hub/en/guides/repository)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizers = {\n",
    "    \n",
    "    # LLM - TikToken\n",
    "    \"GPT 3.5\": \"gpt-3.5-turbo\",\n",
    "    \"GPT 4\": \"gpt-4\",\n",
    "    \n",
    "    # LLM - Hugging Face / transformers\n",
    "    \"Gemma\": \"google/gemma-7b\",\n",
    "    \"Qwen\": \"Qwen/Qwen2.5-7B\",\n",
    "    \"Falcon\": \"tiiuae/falcon-7b\",\n",
    "    \"Mistral\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"Llama 2\": \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"Llama 3\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"Croissant\": \"croissantllm/CroissantLLMBase\",\n",
    "    \"Lucie\": \"OpenLLM-France/Lucie-7B\",  # -> https://huggingface.co/OpenLLM-France/Lucie-7B\n",
    "    \"Bloom\": \"bigscience/bloom-7b1\",\n",
    "    \"Olmo 2\": \"allenai/OLMo-2-1124-7B-Instruct\",\n",
    "    \"C4\": \"CohereForAI/c4ai-command-r-plus\",\n",
    "    \"Aya\": \"CohereForAI/aya-expanse-8b\",\n",
    "    \"Jais\": \"inceptionai/jais-adapted-7b-chat\",\n",
    "    \"EuroLLM\": \"utter-project/EuroLLM-9B\",\n",
    "\n",
    "    # ASR - Hugging Face / transformers\n",
    "    \"Whisper\": \"openai/whisper-large-v3\",\n",
    "    # ASR - Hugging Face / nemo\n",
    "    \"Parakeet\": \"nvidia/parakeet-ctc-1.1b\", # same as \"nvidia/parakeet-rnnt-1.1b\",\n",
    "    \"Perruche\": \"/home/jlouradour/projects/Parakeet/tokenizer_spe_bpe_v1024/tokenizer.model\", # /!!!!\\\\\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** some models (like [Llama](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)) might require to fill some agreements form to have access to the model (with your Hugging Face account).<br>\n",
    "In case of problem, you should have an explicit message with the URL to visit (and fill the form) to get access to the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helpers to load and play with tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy # text encoding transformations\n",
    "import pandas as pd\n",
    "\n",
    "def load_tokenizer(name):\n",
    "    \"\"\"\n",
    "    Load a tokenizer by name.\n",
    "    \"\"\"\n",
    "    # Conform name\n",
    "    global my_tokenizers\n",
    "    if name in my_tokenizers:\n",
    "        name = my_tokenizers[name]\n",
    "\n",
    "    # Load with the right library\n",
    "    # - TikToken\n",
    "    if name.lower().startswith(\"gpt\"):\n",
    "        import tiktoken\n",
    "        return tiktoken.encoding_for_model(name.lower())\n",
    "    # - SentencePiece\n",
    "    elif name.endswith(\".model\"):\n",
    "        import sentencepiece as spm\n",
    "        return spm.SentencePieceProcessor(model_file=name)\n",
    "    # - NeMo\n",
    "    elif name.lower().startswith(\"nvidia\"):\n",
    "        import nemo.collections.asr as nemo_asr\n",
    "        import logging\n",
    "        logging.getLogger('nemo_logger').setLevel(logging.ERROR)\n",
    "        if \"ctc\" in name:\n",
    "            nemo_model_class = nemo_asr.models.EncDecCTCModelBPE\n",
    "        elif \"rnn\" in name:\n",
    "            nemo_model_class = nemo_asr.models.EncDecRNNTBPEModel\n",
    "        else:\n",
    "            raise NotImplementedError(f\"NeMo model '{name}' not supported\")\n",
    "        model = nemo_model_class.from_pretrained(name)\n",
    "        return model.tokenizer.tokenizer\n",
    "    # - Transformers\n",
    "    else:\n",
    "        import transformers\n",
    "        try:\n",
    "            return transformers.AutoTokenizer.from_pretrained(name, trust_remote_code=True)\n",
    "        except Exception as err:\n",
    "            # Report the name in case of error\n",
    "            raise RuntimeError(f\"Could not load tokenizer '{name}': {err}\") from err\n",
    "\n",
    "def load_tokenizer_with_cache(name):\n",
    "    \"\"\"\n",
    "    Load a tokenizer by name, with cache.\n",
    "    \"\"\"\n",
    "    global _loaded_tokenizers\n",
    "    if name not in _loaded_tokenizers:\n",
    "        _loaded_tokenizers[name] = load_tokenizer(name)\n",
    "    return _loaded_tokenizers[name] \n",
    "\n",
    "if \"_loaded_tokenizers\" not in globals():\n",
    "    _loaded_tokenizers = {}\n",
    "\n",
    "\n",
    "def encode_decode(tokenizer, text, add_special_tokens=True, use_internal_tokens=False):\n",
    "    \"\"\"\n",
    "    Round-trip testing:\n",
    "    Encode an input text into a list of tokens, and decode the tokens back to a string.\n",
    "    \"\"\"\n",
    "    if isinstance(tokenizer, str):\n",
    "        tokenizer = load_tokenizer_with_cache(tokenizer)\n",
    "\n",
    "    if \"encode_batch\" in dir(tokenizer):\n",
    "        # TikToken\n",
    "        tokens = tokenizer.encode_batch(\n",
    "            [text],\n",
    "            allowed_special=\"all\" if add_special_tokens else set(),\n",
    "            disallowed_special=()\n",
    "        )[0]\n",
    "        if hasattr(tokens, \"ids\"):\n",
    "            tokens = tokens.ids\n",
    "        if use_internal_tokens and hasattr(tokenizer, \"id_to_token\"):\n",
    "            tokens_strings = [tokenizer.id_to_token(t) for t in tokens]\n",
    "        else:\n",
    "            tokens_strings = [tokenizer.decode([t]) for t in tokens]\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "    elif \"SentencePiece\" in type(tokenizer).__name__:\n",
    "        # SentencePiece\n",
    "        tokens = tokenizer.encode(text,\n",
    "            add_bos=add_special_tokens,\n",
    "            add_eos=add_special_tokens,\n",
    "            emit_unk_piece=add_special_tokens,\n",
    "        )\n",
    "        tokens_strings = tokenizer.encode(text,\n",
    "            out_type=str,\n",
    "            emit_unk_piece=add_special_tokens,\n",
    "        )\n",
    "        if add_special_tokens:\n",
    "            assert len(tokens) == len(tokens_strings) + 2\n",
    "            if tokens[0] >= 0:\n",
    "                tokens_strings = [\"<BOS>\"] + tokens_strings\n",
    "            else:\n",
    "                tokens = tokens[1:]\n",
    "            if tokens[-1] >= 0:\n",
    "                tokens_strings += [\"<EOS>\"]\n",
    "            else:\n",
    "                tokens = tokens[:-1]\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "    else:\n",
    "        # transformers / tokenizers\n",
    "        tokenizer.add_eos_token = bool(add_special_tokens)\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=add_special_tokens)\n",
    "        # tokens_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "        tokens_strings = [tokenizer.decode(t, skip_special_tokens=False) for t in tokens]\n",
    "        decoded = tokenizer.decode(tokens, skip_special_tokens=not add_special_tokens)\n",
    "\n",
    "    # Normalize for display\n",
    "    tokens_strings = normalize_for_display(tokens_strings, is_token=True)\n",
    "    norm_decoded = normalize_for_display(decoded)\n",
    "\n",
    "    return tokens, tokens_strings, decoded, norm_decoded\n",
    "\n",
    "\n",
    "def vocabulary_size(tokenizer):\n",
    "    \"\"\"\n",
    "    Get the vocabulary size of a tokenizer.\n",
    "    \"\"\"\n",
    "    if isinstance(tokenizer, str):\n",
    "        tokenizer = load_tokenizer_with_cache(tokenizer)\n",
    "    \n",
    "    N = None\n",
    "    for attr_name in \"n_vocab\", \"vocab_size\":\n",
    "        if attr_name in dir(tokenizer):\n",
    "            N = getattr(tokenizer, attr_name)\n",
    "            if not isinstance(N, int):\n",
    "                assert callable(N)\n",
    "                N = N()\n",
    "            break\n",
    "    if not N:\n",
    "        raise NotImplementedError(f\"Vocabulary not supported for tokenizer '{tokenizer}'\")\n",
    "    return N\n",
    "\n",
    "\n",
    "def get_vocabulary(tokenizer):\n",
    "    \"\"\"\n",
    "    Get the vocabulary of a tokenizer.\n",
    "    \"\"\"\n",
    "    if isinstance(tokenizer, str):\n",
    "        tokenizer = load_tokenizer_with_cache(tokenizer)\n",
    "\n",
    "    N = vocabulary_size(tokenizer)\n",
    "    try:\n",
    "        tokens_str = [tokenizer.decode([t], skip_special_tokens=False) for t in range(N)]\n",
    "    except Exception as err:\n",
    "        tokens_str = [normalize_for_display(tokenizer.decode([t]), is_token=True) for t in range(N)]\n",
    "    if \"id_to_piece\" in dir(tokenizer):\n",
    "        tokens_str_check = [normalize_for_display(tokenizer.id_to_piece(t), is_token=True) for t in range(N)]\n",
    "        for i, (t1, t2) in enumerate(zip(tokens_str, tokens_str_check)):\n",
    "            if t1 != t2:\n",
    "                if not t1.startswith(\"▁\") and t2.startswith(\"▁\"):\n",
    "                    tokens_str[i] = \"▁\" + tokens_str[i]\n",
    "    \n",
    "    tokens_str = [normalize_for_display(tok, is_token=True) for tok in tokens_str]\n",
    "    return tokens_str\n",
    "\n",
    "\n",
    "def sorted_vocabulary(vocab):\n",
    "    if isinstance(vocab, str):\n",
    "        vocab = get_vocabulary(vocab)\n",
    "    tokens_by_length = {}\n",
    "    for token in sorted(vocab):\n",
    "        nchars = len(token.lstrip(\"▁\"))\n",
    "        sow = token.startswith(\"▁\")\n",
    "        key = (nchars, sow)\n",
    "        if key not in tokens_by_length:\n",
    "            tokens_by_length[key] = []\n",
    "        tokens_by_length[key].append(token)\n",
    "\n",
    "    data = []\n",
    "    for (nchars, sow) in sorted(tokens_by_length):\n",
    "        tokens = sorted(tokens_by_length[(nchars, sow)])\n",
    "        data.append({\n",
    "            \"nchars\": nchars,\n",
    "            \"start\": \"▁\" if sow else \"#\",\n",
    "            \"#tokens\": len(tokens),\n",
    "            \"tokens\": tokens # normalize_for_display(tokens, is_token=True),\n",
    "        })\n",
    "\n",
    "    data.append({\n",
    "        \"nchars\": \"TOTAL\",\n",
    "        \"start\": \"\",\n",
    "        \"#tokens\": sum(d[\"#tokens\"] for d in data),\n",
    "        \"tokens\": \"\",\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Will be defined later\n",
    "if \"normalize_for_display\" not in globals():\n",
    "    def normalize_for_display(text, *kargs, **kwargs):\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some helpers to display things nicely. <br>\n",
    "There is a special treatment for string including both Arabic and latin characters (so that characters are displayed in the right order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☪ Test: Fix of display for text with Arabic and code-switching ☪\n",
      "------------------------------------------------------------------\n",
      "➡️ Original string\n",
      "مرحباً Jean-Pierre، كيف حالك؟\n",
      "➡️ String for display\n",
      "‏، كيف حالك؟‎Jean-Pierre ‏مرحباً \n",
      "➡️ Word decomposition (of original string)\n",
      "  1- ‏مرحباً\n",
      "  2- ‏،‎Jean-Pierre\n",
      "  3- ‏كيف\n",
      "  4- ‏حالك؟\n",
      "\n",
      " ‏، كيف حالك؟‎Jean-Pierre\n",
      "\n",
      " ‎Jean-Pierre ‏مرحباً \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Display</th>\n",
       "      <th>Display as tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحباً Jean-Pierre، كيف حالك؟</td>\n",
       "      <td>‏، كيف حالك؟‎Jean-Pierre ‏مرحباً</td>\n",
       "      <td>‏،┃كيف┃حالك؟‎Jean-Pierre┃‏مرحباً</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jean-Pierre، كيف حالك؟</td>\n",
       "      <td>‏، كيف حالك؟‎Jean-Pierre</td>\n",
       "      <td>‏،┃كيف┃حالك؟‎Jean-Pierre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مرحباً Jean-Pierre</td>\n",
       "      <td>‎Jean-Pierre ‏مرحباً</td>\n",
       "      <td>‎Jean-Pierre┃‏مرحباً</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Original                       Display                             \\\n",
       "0  مرحباً Jean-Pierre، كيف حالك؟  ‏، كيف حالك؟‎Jean-Pierre ‏مرحباً    \n",
       "1         Jean-Pierre، كيف حالك؟           ‏، كيف حالك؟‎Jean-Pierre   \n",
       "2             مرحباً Jean-Pierre              ‎Jean-Pierre ‏مرحباً    \n",
       "\n",
       "  Display as tokens                  \n",
       "0  ‏،┃كيف┃حالك؟‎Jean-Pierre┃‏مرحباً  \n",
       "1          ‏،┃كيف┃حالك؟‎Jean-Pierre  \n",
       "2              ‎Jean-Pierre┃‏مرحباً  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_for_display(text, is_token=False, fix_arabic=True):\n",
    "    \"\"\"\n",
    "    Normalize token/text for display.\n",
    "    \"\"\"\n",
    "    if isinstance(text, list):\n",
    "        \n",
    "        if is_token:\n",
    "            token_list = [normalize_for_display(t, is_token=is_token, fix_arabic=False) for t in text]\n",
    "            # Add a separator between tokens\n",
    "            return fix_arabic_display(\"┃\".join(token_list))\n",
    "        else:\n",
    "            # Map function on each element\n",
    "            return [normalize_for_display(t, is_token=is_token, fix_arabic=fix_arabic) for t in text]\n",
    "    \n",
    "    # Escape line breaks and tabs\n",
    "    text = text \\\n",
    "        .replace(\"\\n\", \"\\\\n\") \\\n",
    "        .replace(\"\\t\", \"\\\\t\") \\\n",
    "        .replace(\"\\r\", \"\\\\r\")\n",
    "    \n",
    "    if is_token:\n",
    "        # Standard representation of whitespace\n",
    "        text = text \\\n",
    "            .replace(\" \", \"▁\") \\\n",
    "            .replace(\" \", \"\\\\▁\") \\\n",
    "            .replace(\"Ġ\", \"▁\")\n",
    "    \n",
    "    # For non-ASCII characters that are NOT encoded in UTF-8\n",
    "    # \"Ã©\" -> \"é\", ...\n",
    "    # It happens with the token \"representation strings\" for byte-level models like Bloom, Qwen, Falcon, Llama 2, Olmo 2, C4, Aya \n",
    "    # if tokens are not decoded (should be deprecated with correct implementation)\n",
    "    if \"Ã\" in text or \"â\" in text:\n",
    "        text = ftfy.fix_text(text, normalization=\"NFC\")\n",
    "    \n",
    "    # Special tokens\n",
    "    for special_in, special_out in {\n",
    "        # Mistral, Llama 2\n",
    "        \"<s>\": \"<BOS>\", \"</s>\": \"<EOS>\",\n",
    "        # C4, Aya\n",
    "        \"<BOS_TOKEN>\": \"<BOS>\", \"<|END_OF_TURN_TOKEN|>\": \"<EOS>\",\n",
    "        # Llama 3\n",
    "        \"<|begin_of_text|>\": \"<BOS>\", \"<|end_of_text|>\": \"<EOS>\",\n",
    "        # Gemma\n",
    "        \"<bos>\": \"<BOS>\", \"<eos>\": \"<EOS>\",\n",
    "        # Whisper\n",
    "        \"<|startoftranscript|>\": \"<BOS>\", \"<|endoftext|>\": \"<EOS>\",\n",
    "        # Nemo\n",
    "        \"⁇\": \"<UNK>\",\n",
    "        # misc. tags\n",
    "        \"<|\": \"<\", \"|>\": \">\",\n",
    "    }.items():\n",
    "        text = text.replace(special_in, special_out)\n",
    "    \n",
    "    if fix_arabic:\n",
    "        text = fix_arabic_display(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# from bidi.algorithm import get_display # Did not find a good thing in python-bidi (?)\n",
    "\n",
    "# Unicode characters for Right-to-Left Mark (RLM) and Left-to-Right Mark (LRM)\n",
    "_RLM = '\\u200F'\n",
    "_LRM = '\\u200E'\n",
    "\n",
    "def is_separator(char):\n",
    "    return char in \"┃\"\n",
    "\n",
    "def is_arabic(char):\n",
    "    return ord(char) in range(0x600, 0x6ff) or is_separator(char)\n",
    "\n",
    "def is_word(char):\n",
    "    return char.isalpha() or char in \"؟،\"\n",
    "\n",
    "def contains_arabic(text):\n",
    "    return any(is_arabic(c) for c in text)\n",
    "\n",
    "def contains_latin(text):\n",
    "    return any(not is_arabic(c) and is_word(c) for c in text)\n",
    "\n",
    "def fix_arabic_display(input, verbose=False):\n",
    "    \"\"\"\n",
    "    Format a string with Arabic and latin characters (code-switching) for a good display in the notebook.\n",
    "    \"\"\"\n",
    "    if isinstance(input, list):\n",
    "        return [fix_arabic_display(i) for i in input]\n",
    "    \n",
    "    # Ignore already converted text\n",
    "    if _RLM in input or _LRM in input:\n",
    "        return input\n",
    "    # Convert only code-switching text\n",
    "    if not contains_arabic(input) or not contains_latin(input):\n",
    "        if contains_arabic(input):\n",
    "            input = _RLM + input\n",
    "        return input\n",
    "    \n",
    "    if not input: return \"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Fixing display for: {input}\")\n",
    "\n",
    "    is_current_arabic = is_arabic(input[0])\n",
    "    chunks_by_language = [\n",
    "        _RLM if is_current_arabic else _LRM\n",
    "    ]\n",
    "    is_space = False\n",
    "    must_add_extra_space = False\n",
    "\n",
    "\n",
    "    for c in input:\n",
    "        was_previous_arabic = is_current_arabic\n",
    "        is_current_arabic = is_arabic(c)\n",
    "        # Special treatment for space (have to be in Arabic chunks)\n",
    "        was_previous_space = is_space\n",
    "        is_space = (c == \" \")\n",
    "        if is_space:\n",
    "            is_current_arabic = True\n",
    "        if is_current_arabic != was_previous_arabic:\n",
    "            if must_add_extra_space:\n",
    "                chunks_by_language[-1] += \" \"\n",
    "            # Add direction switch mark before the character\n",
    "            chunks_by_language.append(_RLM if is_current_arabic else _LRM)\n",
    "            # Add an extra space before the code-switching\n",
    "            must_add_extra_space = was_previous_space\n",
    "\n",
    "        chunks_by_language[-1] += c\n",
    "\n",
    "    if must_add_extra_space:\n",
    "        chunks_by_language[-1] += \" \"\n",
    "\n",
    "    # Reverse the chunk order\n",
    "    chunks_by_language = chunks_by_language[::-1]\n",
    "\n",
    "    # Put separators on the other side (they are affected to Arabic segments)\n",
    "    for i, chunk in enumerate(chunks_by_language):\n",
    "        if is_separator(chunk[-1]) and not is_separator(chunk[0]):\n",
    "            chunks_by_language[i] = chunk[-1] + chunk[:-1]\n",
    "\n",
    "    return \"\".join(chunks_by_language)\n",
    "\n",
    "\n",
    "if \"TEST\":\n",
    "    title = \"☪ Test: Fix of display for text with Arabic and code-switching ☪\"\n",
    "    print(f\"{title}\\n\" + \"-\"*(len(title)+2))\n",
    "\n",
    "    input = \"مرحباً Jean-Pierre، كيف حالك؟\"\n",
    "    inputs = [\n",
    "        input,\n",
    "        \"Jean-Pierre، كيف حالك؟\",\n",
    "        \"مرحباً Jean-Pierre\",\n",
    "        # input.replace(\"،\", \",\").replace(\"؟\", \"?\"),\n",
    "        # \"مرحباً أحمد، كيف حالك؟\",\n",
    "        # \"Bonjour Jean-Pierre, comment ça va?\",\n",
    "    ]\n",
    "    inputs_for_display = [fix_arabic_display(i) for i in inputs]\n",
    "\n",
    "    for i, (input, display_input) in enumerate(zip(inputs, inputs_for_display)):\n",
    "        if i > 0:\n",
    "            print(\"\\n\", display_input)\n",
    "            continue\n",
    "\n",
    "        print(\"➡️ Original string\")\n",
    "        print(input)\n",
    "        print(\"➡️ String for display\")\n",
    "        print(display_input)\n",
    "        print(\"➡️ Word decomposition (of original string)\")\n",
    "        for i, word in enumerate(input.split()):\n",
    "            print(f\" {i+1:2d}- {fix_arabic_display(word)}\")\n",
    "\n",
    "        # # Uncomment for debugging (see all characters encoded, one by line)\n",
    "        # print(\"➡️ Character decomposition (of string for display):\")\n",
    "        # for i, c in enumerate(display_input):\n",
    "        #     c = c.replace(_RLM, \"<RLM>\").replace(_LRM, \"<LRM>\")\n",
    "        #     print(f\" {i+1:2d}- {c}\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Original\": inputs,\n",
    "    \"Display\": inputs_for_display,\n",
    "    \"Display as tokens\": [normalize_for_display(input.split(), is_token=True) for input in inputs],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the tokenizer download / loading (which can be long 🥱)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "for tokenizer in my_tokenizers:\n",
    "    load_tokenizer_with_cache(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test encoding and decoding with a simple string with a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ [1, 8396, 29926, 473, 818, 9411, 1738, 2]\n",
      "➡️ ‎<EOS>┃‏‎!┃‏‎tous┃‏‎à┃‏‎our┃‏‎j┃‏‎Bon┃‏‎<BOS>\n",
      "➡️ <s> Bonjour à tous !</s>\n",
      "➡️ <BOS> Bonjour à tous !<EOS>\n"
     ]
    }
   ],
   "source": [
    "for output in encode_decode(\n",
    "    \"Llama 2\", # tokenizer\n",
    "    \"Bonjour à tous !\" # input\n",
    "    ):\n",
    "    print(\"➡️\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👀 See what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pandas` library (just to diplay tables later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.colheader_justify', 'left') # WTF ?\n",
    "# pd.set_option('display.show_index', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 𖡎 See how tokenizers do tokenize strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>size</th>\n",
       "      <th>#tokens</th>\n",
       "      <th>fert.</th>\n",
       "      <th>encoded tokens</th>\n",
       "      <th>round-trip</th>\n",
       "      <th>decoded text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>100.3k</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>‎▁!┃‏‎as┃‏‎-b┃‏‎▁là┃‏‎'en┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais</td>\n",
       "      <td>✅</td>\n",
       "      <td>Mais, mais… vas t'en là-bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT 4</td>\n",
       "      <td>100.3k</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>‎▁!┃‏‎as┃‏‎-b┃‏‎▁là┃‏‎'en┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais</td>\n",
       "      <td>✅</td>\n",
       "      <td>Mais, mais… vas t'en là-bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemma</td>\n",
       "      <td>256k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏‎▁!┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais┃‏‎&lt;BOS&gt;</td>\n",
       "      <td>✅</td>\n",
       "      <td>&lt;BOS&gt;Mais, mais… vas t'en là-bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>151.6k</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>‎▁!┃‏‎as┃‏‎-b┃‏‎▁là┃‏‎'en┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais</td>\n",
       "      <td>✅</td>\n",
       "      <td>Mais, mais… vas t'en là-bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Falcon</td>\n",
       "      <td>65k</td>\n",
       "      <td>13</td>\n",
       "      <td>2.2</td>\n",
       "      <td>‎!┃‏‎▁┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais</td>\n",
       "      <td>✅</td>\n",
       "      <td>Mais, mais… vas t'en là-bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mistral</td>\n",
       "      <td>32k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏‎!┃‏‎bas┃‏‎-┃‏‎là┃‏‎en┃‏‎'┃‏‎t┃‏‎vas┃‏‎…┃‏‎mais┃‏‎,┃‏‎Mais┃‏‎&lt;BOS&gt;</td>\n",
       "      <td>✅</td>\n",
       "      <td>&lt;BOS&gt; Mais, mais… vas t'en là-bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama 2</td>\n",
       "      <td>32k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏‎!┃‏‎bas┃‏‎-┃‏‎là┃‏‎en┃‏‎'┃‏‎t┃‏‎vas┃‏‎…┃‏‎mais┃‏‎,┃‏‎Mais┃‏‎&lt;BOS&gt;</td>\n",
       "      <td>✅</td>\n",
       "      <td>&lt;BOS&gt; Mais, mais… vas t'en là-bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>128k</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>‎!┃‏‎as┃‏‎-b┃‏‎▁là┃‏‎'en┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais┃‏‎&lt;BOS&gt;</td>\n",
       "      <td>❌</td>\n",
       "      <td>&lt;BOS&gt;Mais, mais… vas t'en là-bas!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Croissant</td>\n",
       "      <td>32k</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏‎!┃‏‎-bas┃‏‎là┃‏‎en┃‏‎t'┃‏‎vas┃‏‎...┃‏‎mais┃‏‎Mais,┃‏‎&lt;BOS&gt;</td>\n",
       "      <td>❌</td>\n",
       "      <td>&lt;BOS&gt; Mais, mais... vas t'en là-bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lucie</td>\n",
       "      <td>65k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏‎!┃‏‎bas┃‏‎-┃‏‎là┃‏‎en┃‏‎'┃‏‎t┃‏‎vas┃‏‎…┃‏‎mais┃‏‎,┃‏‎Mais┃‏‎&lt;BOS&gt;</td>\n",
       "      <td>✅</td>\n",
       "      <td>&lt;BOS&gt; Mais, mais… vas t'en là-bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bloom</td>\n",
       "      <td>250.7k</td>\n",
       "      <td>8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>‎▁!┃‏‎▁là-bas┃‏‎▁t'en┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais</td>\n",
       "      <td>✅</td>\n",
       "      <td>Mais, mais… vas t'en là-bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Olmo 2</td>\n",
       "      <td>100.3k</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>‎▁!┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais</td>\n",
       "      <td>✅</td>\n",
       "      <td>Mais, mais… vas t'en là-bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C4</td>\n",
       "      <td>255k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏‎▁!┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais┃‏‎&lt;BOS&gt;</td>\n",
       "      <td>✅</td>\n",
       "      <td>&lt;BOS&gt;Mais, mais… vas t'en là-bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Aya</td>\n",
       "      <td>255k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏‎▁!┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais┃‏‎&lt;BOS&gt;</td>\n",
       "      <td>✅</td>\n",
       "      <td>&lt;BOS&gt;Mais, mais… vas t'en là-bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Jais</td>\n",
       "      <td>64k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏‎!┃‏‎bas┃‏‎-┃‏‎là┃‏‎en┃‏‎'┃‏‎t┃‏‎vas┃‏‎…┃‏‎mais┃‏‎,┃‏‎Mais┃‏‎&lt;BOS&gt;</td>\n",
       "      <td>✅</td>\n",
       "      <td>&lt;BOS&gt; Mais, mais… vas t'en là-bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EuroLLM</td>\n",
       "      <td>128k</td>\n",
       "      <td>16</td>\n",
       "      <td>2.7</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏‎!┃‏‎bas┃‏‎-┃‏‎là┃‏‎en┃‏‎'┃‏‎t┃‏‎vas┃‏‎�┃‏‎�┃‏‎�┃‏‎mais┃‏‎,┃‏‎Mais┃‏‎&lt;BOS&gt;</td>\n",
       "      <td>✅</td>\n",
       "      <td>&lt;BOS&gt; Mais, mais… vas t'en là-bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Whisper</td>\n",
       "      <td>50.3k</td>\n",
       "      <td>16</td>\n",
       "      <td>2.7</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏‎!┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎ais┃‏‎M┃‏‎&lt;notimestamps&gt;┃‏‎&lt;BOS&gt;</td>\n",
       "      <td>❌</td>\n",
       "      <td>&lt;BOS&gt;&lt;notimestamps&gt;Mais, mais… vas t'en là-bas!&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Parakeet</td>\n",
       "      <td>1024</td>\n",
       "      <td>17</td>\n",
       "      <td>2.8</td>\n",
       "      <td>‎&lt;unk&gt;┃‏‎▁┃‏‎as┃‏‎b┃‏‎&lt;unk&gt;┃‏‎▁l┃‏‎en┃‏‎'┃‏‎▁t┃‏‎as┃‏‎▁v┃‏‎&lt;unk&gt;┃‏‎is┃‏‎▁ma┃‏‎&lt;unk&gt;┃‏‎is┃‏‎▁ma</td>\n",
       "      <td>❌</td>\n",
       "      <td>mais &lt;UNK&gt;  mais &lt;UNK&gt;  vas t'en l &lt;UNK&gt; bas  &lt;UNK&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Perruche</td>\n",
       "      <td>1024</td>\n",
       "      <td>15</td>\n",
       "      <td>2.5</td>\n",
       "      <td>‎&lt;unk&gt;┃‏‎▁┃‏‎as┃‏‎b┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎as┃‏‎▁v┃‏‎&lt;unk&gt;┃‏‎▁mais┃‏‎&lt;unk&gt;┃‏‎▁mais</td>\n",
       "      <td>❌</td>\n",
       "      <td>mais &lt;UNK&gt;  mais &lt;UNK&gt;  vas t'en là-bas  &lt;UNK&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenizer  size     #tokens  fert.  \\\n",
       "0     GPT 3.5  100.3k  11       1.8     \n",
       "1       GPT 4  100.3k  11       1.8     \n",
       "2       Gemma    256k  14       2.3     \n",
       "3        Qwen  151.6k  11       1.8     \n",
       "4      Falcon     65k  13       2.2     \n",
       "5     Mistral     32k  14       2.3     \n",
       "6     Llama 2     32k  14       2.3     \n",
       "7     Llama 3    128k  12       2.0     \n",
       "8   Croissant     32k  11       1.8     \n",
       "9       Lucie     65k  14       2.3     \n",
       "10      Bloom  250.7k   8       1.3     \n",
       "11     Olmo 2  100.3k  12       2.0     \n",
       "12         C4    255k  14       2.3     \n",
       "13        Aya    255k  14       2.3     \n",
       "14       Jais     64k  14       2.3     \n",
       "15    EuroLLM    128k  16       2.7     \n",
       "16    Whisper   50.3k  16       2.7     \n",
       "17   Parakeet    1024  17       2.8     \n",
       "18   Perruche    1024  15       2.5     \n",
       "\n",
       "   encoded tokens                                                                                       \\\n",
       "0                                         ‎▁!┃‏‎as┃‏‎-b┃‏‎▁là┃‏‎'en┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais   \n",
       "1                                         ‎▁!┃‏‎as┃‏‎-b┃‏‎▁là┃‏‎'en┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais   \n",
       "2                      ‎<EOS>┃‏‎▁!┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais┃‏‎<BOS>   \n",
       "3                                         ‎▁!┃‏‎as┃‏‎-b┃‏‎▁là┃‏‎'en┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais   \n",
       "4                                   ‎!┃‏‎▁┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais   \n",
       "5                           ‎<EOS>┃‏‎!┃‏‎bas┃‏‎-┃‏‎là┃‏‎en┃‏‎'┃‏‎t┃‏‎vas┃‏‎…┃‏‎mais┃‏‎,┃‏‎Mais┃‏‎<BOS>   \n",
       "6                           ‎<EOS>┃‏‎!┃‏‎bas┃‏‎-┃‏‎là┃‏‎en┃‏‎'┃‏‎t┃‏‎vas┃‏‎…┃‏‎mais┃‏‎,┃‏‎Mais┃‏‎<BOS>   \n",
       "7                                  ‎!┃‏‎as┃‏‎-b┃‏‎▁là┃‏‎'en┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais┃‏‎<BOS>   \n",
       "8                                  ‎<EOS>┃‏‎!┃‏‎-bas┃‏‎là┃‏‎en┃‏‎t'┃‏‎vas┃‏‎...┃‏‎mais┃‏‎Mais,┃‏‎<BOS>   \n",
       "9                           ‎<EOS>┃‏‎!┃‏‎bas┃‏‎-┃‏‎là┃‏‎en┃‏‎'┃‏‎t┃‏‎vas┃‏‎…┃‏‎mais┃‏‎,┃‏‎Mais┃‏‎<BOS>   \n",
       "10                                                 ‎▁!┃‏‎▁là-bas┃‏‎▁t'en┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais   \n",
       "11                                     ‎▁!┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais   \n",
       "12                     ‎<EOS>┃‏‎▁!┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais┃‏‎<BOS>   \n",
       "13                     ‎<EOS>┃‏‎▁!┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎Mais┃‏‎<BOS>   \n",
       "14                          ‎<EOS>┃‏‎!┃‏‎bas┃‏‎-┃‏‎là┃‏‎en┃‏‎'┃‏‎t┃‏‎vas┃‏‎…┃‏‎mais┃‏‎,┃‏‎Mais┃‏‎<BOS>   \n",
       "15                  ‎<EOS>┃‏‎!┃‏‎bas┃‏‎-┃‏‎là┃‏‎en┃‏‎'┃‏‎t┃‏‎vas┃‏‎�┃‏‎�┃‏‎�┃‏‎mais┃‏‎,┃‏‎Mais┃‏‎<BOS>   \n",
       "16  ‎<EOS>┃‏‎!┃‏‎bas┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎▁vas┃‏‎…┃‏‎▁mais┃‏‎,┃‏‎ais┃‏‎M┃‏‎<notimestamps>┃‏‎<BOS>   \n",
       "17      ‎<unk>┃‏‎▁┃‏‎as┃‏‎b┃‏‎<unk>┃‏‎▁l┃‏‎en┃‏‎'┃‏‎▁t┃‏‎as┃‏‎▁v┃‏‎<unk>┃‏‎is┃‏‎▁ma┃‏‎<unk>┃‏‎is┃‏‎▁ma   \n",
       "18               ‎<unk>┃‏‎▁┃‏‎as┃‏‎b┃‏‎-┃‏‎▁là┃‏‎en┃‏‎'┃‏‎▁t┃‏‎as┃‏‎▁v┃‏‎<unk>┃‏‎▁mais┃‏‎<unk>┃‏‎▁mais   \n",
       "\n",
       "   round-trip decoded text                                           \n",
       "0   ✅                                 Mais, mais… vas t'en là-bas !  \n",
       "1   ✅                                 Mais, mais… vas t'en là-bas !  \n",
       "2   ✅                       <BOS>Mais, mais… vas t'en là-bas !<EOS>  \n",
       "3   ✅                                 Mais, mais… vas t'en là-bas !  \n",
       "4   ✅                                 Mais, mais… vas t'en là-bas !  \n",
       "5   ✅                      <BOS> Mais, mais… vas t'en là-bas !<EOS>  \n",
       "6   ✅                      <BOS> Mais, mais… vas t'en là-bas !<EOS>  \n",
       "7   ❌                             <BOS>Mais, mais… vas t'en là-bas!  \n",
       "8   ❌                    <BOS> Mais, mais... vas t'en là-bas !<EOS>  \n",
       "9   ✅                      <BOS> Mais, mais… vas t'en là-bas !<EOS>  \n",
       "10  ✅                                 Mais, mais… vas t'en là-bas !  \n",
       "11  ✅                                 Mais, mais… vas t'en là-bas !  \n",
       "12  ✅                       <BOS>Mais, mais… vas t'en là-bas !<EOS>  \n",
       "13  ✅                       <BOS>Mais, mais… vas t'en là-bas !<EOS>  \n",
       "14  ✅                      <BOS> Mais, mais… vas t'en là-bas !<EOS>  \n",
       "15  ✅                      <BOS> Mais, mais… vas t'en là-bas !<EOS>  \n",
       "16  ❌          <BOS><notimestamps>Mais, mais… vas t'en là-bas!<EOS>  \n",
       "17  ❌          mais <UNK>  mais <UNK>  vas t'en l <UNK> bas  <UNK>   \n",
       "18  ❌               mais <UNK>  mais <UNK>  vas t'en là-bas  <UNK>   "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def test_tokenizers_on_text(\n",
    "        text,\n",
    "        tokenizers=my_tokenizers,\n",
    "        display_vocabulary_size=True,\n",
    "        display_fertility=True,\n",
    "        display_round_trip_result=True,\n",
    "        ):\n",
    "    # Build a table with the encoded and decoded text for each tokenizer\n",
    "    all_data = []\n",
    "    for tokenizer in tokenizers:\n",
    "        tokens, tokens_str, decoded_raw, decoded = encode_decode(tokenizer, text)\n",
    "        N = vocabulary_size(tokenizer)\n",
    "        data = {\n",
    "            \"tokenizer\": tokenizer,\n",
    "        }\n",
    "        if display_vocabulary_size:\n",
    "            data[\"size\"] = format_thoushands(N)\n",
    "        if display_fertility:\n",
    "            num_words = len(text.split())\n",
    "            num_tokens = len(tokens)\n",
    "            if tokens_str.startswith(\"<BOS>\"):\n",
    "                num_tokens -= 1\n",
    "            if tokens_str.endswith(\"<EOS>\"):\n",
    "                num_tokens -= 1\n",
    "            data[\"#tokens\"] = num_tokens\n",
    "            data[\"fert.\"] = round(num_tokens / num_words, 1)\n",
    "        data[\"encoded tokens\"]= tokens_str\n",
    "        if display_round_trip_result:\n",
    "            # Remove all tags in brackets (like <BOS>, <EOS>, <UNK>, ...)\n",
    "            normalized_decoded = re.sub(r\"<[^>]*>\", \"\", decoded_raw)\n",
    "            data[\"round-trip\"] = \"✅\" if normalized_decoded.strip() == text.strip() else \"❌\"\n",
    "        data[\"decoded text\"]= decoded\n",
    "        all_data.append(data)\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def format_thoushands(N):\n",
    "    import math\n",
    "    if N < 1100:\n",
    "        return str(N)\n",
    "    if N - 1000 * math.floor(N/1000) < 100:\n",
    "        return f\"{N//1000}k\"\n",
    "    return f\"{N/1000:.1f}k\"\n",
    "    \n",
    "\n",
    "test_tokenizers_on_text(\"Mais, mais… vas t'en là-bas !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 𖡎 See how tokenizers do tokenize strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 See vocabulary of (small) tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>nchars</th>\n",
       "      <th>start</th>\n",
       "      <th>#tokens</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>▁</td>\n",
       "      <td>1</td>\n",
       "      <td>[▁]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#</td>\n",
       "      <td>27</td>\n",
       "      <td>[', a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>▁</td>\n",
       "      <td>25</td>\n",
       "      <td>[▁', ▁a, ▁b, ▁c, ▁d, ▁e, ▁f, ▁g, ▁h, ▁i, ▁j, ▁k, ▁l, ▁m, ▁n, ▁o, ▁p, ▁r, ▁s, ▁t, ▁u, ▁v, ▁w, ▁y, ▁z]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>#</td>\n",
       "      <td>119</td>\n",
       "      <td>[ab, ac, ad, ag, ah, ak, al, am, an, ap, ar, as, at, av, ay, be, ce, ch, ci, ck, co, ct, cy, de, du, ed, ef, el, em, en, ep, er, es, et, ew, fe, ff, ft, ge, gg, gn, ht, ia, ib, ic, id, ie, if, ig, il, im, in, ip, ir, is, it, iv, ix, iz, ke, ks, ld, le, li, ll, ly, me, mo, na, nd, ne, nt, od, og, ol, om, on, oo, op, or, os, ot, ou, ow, oy, pe, ph, pl, pp, ps, pt, qu, ra, re, ri, ro, ru, ry, se, so, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>▁</td>\n",
       "      <td>114</td>\n",
       "      <td>[▁ab, ▁ac, ▁ad, ▁af, ▁ag, ▁ah, ▁al, ▁am, ▁an, ▁ap, ▁ar, ▁as, ▁at, ▁be, ▁bl, ▁bo, ▁br, ▁bu, ▁by, ▁ca, ▁ch, ▁cl, ▁co, ▁cr, ▁de, ▁do, ▁dr, ▁ed, ▁el, ▁em, ▁en, ▁es, ▁eu, ▁ev, ▁ex, ▁fa, ▁fe, ▁fl, ▁fo, ▁fr, ▁gl, ▁go, ▁gr, ▁gu, ▁ha, ▁he, ▁ho, ▁hu, ▁if, ▁im, ▁in, ▁is, ▁it, ▁jo, ▁ke, ▁kn, ▁la, ▁le, ▁li, ▁lo, ▁ma, ▁me, ▁mo, ▁mr, ▁mu, ▁my, ▁ne, ▁no, ▁ob, ▁of, ▁oh, ▁ok, ▁on, ▁op, ▁or, ▁pa, ▁pe, ▁ph, ▁pl, ▁po, ▁pr, ▁qu, ▁ra, ▁re, ▁ro, ▁sa, ▁sc, ▁se, ▁sh, ▁sk, ▁sl, ▁sm, ▁so, ▁sp, ▁st, ▁su, ▁sy, ▁ta, ▁te, ▁th, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>#</td>\n",
       "      <td>151</td>\n",
       "      <td>[ace, ach, ack, act, ade, ady, age, ail, aim, ain, all, als, ame, and, ang, ank, ans, ant, ard, are, ark, ars, art, ary, ase, ash, ass, ast, ate, ath, ave, ber, ble, ced, ces, con, cri, cus, day, der, ead, ect, ell, ens, ent, ere, erm, ers, ert, ess, est, ets, ety, ful, ger, her, hip, ial, ian, ice, ich, ick, ics, ict, ide, ied, ies, iew, iff, igh, ign, ild, ile, ill, ily, ind, ine, ing, ink, int, ion, ire, ise, ish, iss, ist, ite, ith, its, itt, ity, ive, ize, kes, led, les, lic, lud, nce, nds, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>▁</td>\n",
       "      <td>174</td>\n",
       "      <td>[▁acc, ▁act, ▁add, ▁adv, ▁all, ▁and, ▁any, ▁app, ▁are, ▁arg, ▁art, ▁ask, ▁ass, ▁att, ▁aut, ▁bas, ▁bec, ▁beg, ▁beh, ▁bel, ▁bet, ▁big, ▁bit, ▁bra, ▁bre, ▁bus, ▁but, ▁can, ▁car, ▁che, ▁cle, ▁col, ▁com, ▁con, ▁cor, ▁cou, ▁cre, ▁cur, ▁day, ▁dec, ▁def, ▁des, ▁det, ▁did, ▁dis, ▁don, ▁ear, ▁eas, ▁eff, ▁ele, ▁end, ▁eng, ▁ent, ▁exp, ▁ext, ▁fam, ▁far, ▁few, ▁fin, ▁fir, ▁for, ▁fun, ▁gen, ▁get, ▁god, ▁got, ▁gra, ▁had, ▁has, ▁hel, ▁her, ▁him, ▁his, ▁hon, ▁how, ▁hum, ▁ide, ▁imp, ▁inc, ▁ind, ▁inf, ▁ins, ▁int, ▁inv, ▁iss, ▁its, ▁jud, ▁lar, ▁law, ▁leg, ▁let, ▁loc, ▁lot, ▁man, ▁mar, ▁may, ▁med, ▁mem, ▁met, ▁min, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>#</td>\n",
       "      <td>75</td>\n",
       "      <td>[able, ably, aint, ally, alth, ance, ange, arch, ason, atch, ated, ater, ates, ause, blem, body, cept, cess, cial, ence, ense, ents, enty, ered, eric, ever, fore, form, hing, ible, ical, ices, ient, ific, ight, ines, ings, ions, ious, ited, ject, king, llow, ment, nder, ness, ning, olog, oney, onna, osed, ough, ould, ound, ount, ouse, ower, pect, ract, reat, ress, ross, self, stem, ther, ting, ular, ures, vern, vers, very, ving, ward, ways, ween]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>▁</td>\n",
       "      <td>158</td>\n",
       "      <td>[▁able, ▁also, ▁appe, ▁away, ▁back, ▁been, ▁best, ▁book, ▁both, ▁call, ▁came, ▁care, ▁case, ▁cent, ▁char, ▁coll, ▁come, ▁comm, ▁comp, ▁conc, ▁conf, ▁cons, ▁cont, ▁cour, ▁didn, ▁diff, ▁dire, ▁dist, ▁does, ▁done, ▁down, ▁each, ▁even, ▁ever, ▁exam, ▁expl, ▁fact, ▁feel, ▁find, ▁five, ▁form, ▁four, ▁frie, ▁from, ▁full, ▁give, ▁good, ▁hand, ▁happ, ▁hard, ▁have, ▁head, ▁hear, ▁help, ▁here, ▁high, ▁home, ▁hund, ▁inst, ▁into, ▁just, ▁keep, ▁kind, ▁know, ▁last, ▁lead, ▁lear, ▁left, ▁life, ▁like, ▁list, ▁long, ▁look, ▁love, ▁made, ▁make, ▁many, ▁mark, ▁mean, ▁miss, ▁more, ▁most, ▁move, ▁much, ▁must, ▁name, ▁need, ▁next, ▁okay, ▁once, ▁only, ▁open, ▁over, ▁part, ▁pass, ▁peop, ▁pers, ▁plan, ▁play, ▁poss, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>#</td>\n",
       "      <td>26</td>\n",
       "      <td>[ately, ather, ating, ation, ative, atter, ature, ction, erest, ether, ident, ility, iness, ision, ities, ition, ittle, ments, other, ought, ready, thing, ually, uring, ution, velop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>▁</td>\n",
       "      <td>70</td>\n",
       "      <td>[▁about, ▁after, ▁again, ▁allow, ▁being, ▁belie, ▁build, ▁child, ▁claim, ▁clear, ▁could, ▁count, ▁court, ▁creat, ▁doesn, ▁doing, ▁eight, ▁every, ▁exper, ▁first, ▁found, ▁gener, ▁going, ▁gonna, ▁great, ▁house, ▁inter, ▁light, ▁maybe, ▁means, ▁might, ▁money, ▁never, ▁order, ▁other, ▁place, ▁point, ▁power, ▁produ, ▁quest, ▁right, ▁small, ▁somet, ▁stand, ▁start, ▁state, ▁still, ▁thank, ▁their, ▁there, ▁these, ▁thing, ▁think, ▁those, ▁thous, ▁three, ▁today, ▁trans, ▁under, ▁using, ▁video, ▁watch, ▁water, ▁where, ▁which, ▁while, ▁whole, ▁world, ▁would, ▁years]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>#</td>\n",
       "      <td>5</td>\n",
       "      <td>[ations, ention, ertain, ically, idence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>▁</td>\n",
       "      <td>42</td>\n",
       "      <td>[▁&lt;UNK&gt;▁, ▁always, ▁americ, ▁around, ▁before, ▁better, ▁called, ▁change, ▁commun, ▁compan, ▁comple, ▁consid, ▁contin, ▁course, ▁enough, ▁experi, ▁family, ▁follow, ▁govern, ▁happen, ▁having, ▁import, ▁includ, ▁inform, ▁little, ▁making, ▁number, ▁partic, ▁people, ▁person, ▁pretty, ▁really, ▁reason, ▁record, ▁saying, ▁school, ▁second, ▁should, ▁system, ▁things, ▁though, ▁trying]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>#</td>\n",
       "      <td>1</td>\n",
       "      <td>[ational]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>▁</td>\n",
       "      <td>21</td>\n",
       "      <td>[▁already, ▁another, ▁because, ▁believe, ▁between, ▁certain, ▁develop, ▁differe, ▁example, ▁getting, ▁hundred, ▁looking, ▁problem, ▁process, ▁support, ▁thought, ▁through, ▁underst, ▁whether, ▁without, ▁working]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>▁</td>\n",
       "      <td>8</td>\n",
       "      <td>[▁actually, ▁anything, ▁business, ▁interest, ▁probably, ▁question, ▁thousand, ▁together]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>▁</td>\n",
       "      <td>3</td>\n",
       "      <td>[▁different, ▁important, ▁something]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>▁</td>\n",
       "      <td>3</td>\n",
       "      <td>[▁everything, ▁government, ▁understand]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>▁</td>\n",
       "      <td>1</td>\n",
       "      <td>[▁information]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td></td>\n",
       "      <td>1024</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nchars start  #tokens  \\\n",
       "0       0  ▁        1      \n",
       "1       1  #       27      \n",
       "2       1  ▁       25      \n",
       "3       2  #      119      \n",
       "4       2  ▁      114      \n",
       "5       3  #      151      \n",
       "6       3  ▁      174      \n",
       "7       4  #       75      \n",
       "8       4  ▁      158      \n",
       "9       5  #       26      \n",
       "10      5  ▁       70      \n",
       "11      6  #        5      \n",
       "12      6  ▁       42      \n",
       "13      7  #        1      \n",
       "14      7  ▁       21      \n",
       "15      8  ▁        8      \n",
       "16      9  ▁        3      \n",
       "17     10  ▁        3      \n",
       "18     11  ▁        1      \n",
       "19  TOTAL        1024      \n",
       "\n",
       "   tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [▁]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [', a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [▁', ▁a, ▁b, ▁c, ▁d, ▁e, ▁f, ▁g, ▁h, ▁i, ▁j, ▁k, ▁l, ▁m, ▁n, ▁o, ▁p, ▁r, ▁s, ▁t, ▁u, ▁v, ▁w, ▁y, ▁z]  \n",
       "3                                                                                                                                                                                                                                                                                                               [ab, ac, ad, ag, ah, ak, al, am, an, ap, ar, as, at, av, ay, be, ce, ch, ci, ck, co, ct, cy, de, du, ed, ef, el, em, en, ep, er, es, et, ew, fe, ff, ft, ge, gg, gn, ht, ia, ib, ic, id, ie, if, ig, il, im, in, ip, ir, is, it, iv, ix, iz, ke, ks, ld, le, li, ll, ly, me, mo, na, nd, ne, nt, od, og, ol, om, on, oo, op, or, os, ot, ou, ow, oy, pe, ph, pl, pp, ps, pt, qu, ra, re, ri, ro, ru, ry, se, so, ...]  \n",
       "4                                                                                                                                                                                                           [▁ab, ▁ac, ▁ad, ▁af, ▁ag, ▁ah, ▁al, ▁am, ▁an, ▁ap, ▁ar, ▁as, ▁at, ▁be, ▁bl, ▁bo, ▁br, ▁bu, ▁by, ▁ca, ▁ch, ▁cl, ▁co, ▁cr, ▁de, ▁do, ▁dr, ▁ed, ▁el, ▁em, ▁en, ▁es, ▁eu, ▁ev, ▁ex, ▁fa, ▁fe, ▁fl, ▁fo, ▁fr, ▁gl, ▁go, ▁gr, ▁gu, ▁ha, ▁he, ▁ho, ▁hu, ▁if, ▁im, ▁in, ▁is, ▁it, ▁jo, ▁ke, ▁kn, ▁la, ▁le, ▁li, ▁lo, ▁ma, ▁me, ▁mo, ▁mr, ▁mu, ▁my, ▁ne, ▁no, ▁ob, ▁of, ▁oh, ▁ok, ▁on, ▁op, ▁or, ▁pa, ▁pe, ▁ph, ▁pl, ▁po, ▁pr, ▁qu, ▁ra, ▁re, ▁ro, ▁sa, ▁sc, ▁se, ▁sh, ▁sk, ▁sl, ▁sm, ▁so, ▁sp, ▁st, ▁su, ▁sy, ▁ta, ▁te, ▁th, ...]  \n",
       "5                                                                                                                                                                                                           [ace, ach, ack, act, ade, ady, age, ail, aim, ain, all, als, ame, and, ang, ank, ans, ant, ard, are, ark, ars, art, ary, ase, ash, ass, ast, ate, ath, ave, ber, ble, ced, ces, con, cri, cus, day, der, ead, ect, ell, ens, ent, ere, erm, ers, ert, ess, est, ets, ety, ful, ger, her, hip, ial, ian, ice, ich, ick, ics, ict, ide, ied, ies, iew, iff, igh, ign, ild, ile, ill, ily, ind, ine, ing, ink, int, ion, ire, ise, ish, iss, ist, ite, ith, its, itt, ity, ive, ize, kes, led, les, lic, lud, nce, nds, ...]  \n",
       "6                                                                                                       [▁acc, ▁act, ▁add, ▁adv, ▁all, ▁and, ▁any, ▁app, ▁are, ▁arg, ▁art, ▁ask, ▁ass, ▁att, ▁aut, ▁bas, ▁bec, ▁beg, ▁beh, ▁bel, ▁bet, ▁big, ▁bit, ▁bra, ▁bre, ▁bus, ▁but, ▁can, ▁car, ▁che, ▁cle, ▁col, ▁com, ▁con, ▁cor, ▁cou, ▁cre, ▁cur, ▁day, ▁dec, ▁def, ▁des, ▁det, ▁did, ▁dis, ▁don, ▁ear, ▁eas, ▁eff, ▁ele, ▁end, ▁eng, ▁ent, ▁exp, ▁ext, ▁fam, ▁far, ▁few, ▁fin, ▁fir, ▁for, ▁fun, ▁gen, ▁get, ▁god, ▁got, ▁gra, ▁had, ▁has, ▁hel, ▁her, ▁him, ▁his, ▁hon, ▁how, ▁hum, ▁ide, ▁imp, ▁inc, ▁ind, ▁inf, ▁ins, ▁int, ▁inv, ▁iss, ▁its, ▁jud, ▁lar, ▁law, ▁leg, ▁let, ▁loc, ▁lot, ▁man, ▁mar, ▁may, ▁med, ▁mem, ▁met, ▁min, ...]  \n",
       "7                                                                                                                                                                                                                                                                  [able, ably, aint, ally, alth, ance, ange, arch, ason, atch, ated, ater, ates, ause, blem, body, cept, cess, cial, ence, ense, ents, enty, ered, eric, ever, fore, form, hing, ible, ical, ices, ient, ific, ight, ines, ings, ions, ious, ited, ject, king, llow, ment, nder, ness, ning, olog, oney, onna, osed, ough, ould, ound, ount, ouse, ower, pect, ract, reat, ress, ross, self, stem, ther, ting, ular, ures, vern, vers, very, ving, ward, ways, ween]  \n",
       "8   [▁able, ▁also, ▁appe, ▁away, ▁back, ▁been, ▁best, ▁book, ▁both, ▁call, ▁came, ▁care, ▁case, ▁cent, ▁char, ▁coll, ▁come, ▁comm, ▁comp, ▁conc, ▁conf, ▁cons, ▁cont, ▁cour, ▁didn, ▁diff, ▁dire, ▁dist, ▁does, ▁done, ▁down, ▁each, ▁even, ▁ever, ▁exam, ▁expl, ▁fact, ▁feel, ▁find, ▁five, ▁form, ▁four, ▁frie, ▁from, ▁full, ▁give, ▁good, ▁hand, ▁happ, ▁hard, ▁have, ▁head, ▁hear, ▁help, ▁here, ▁high, ▁home, ▁hund, ▁inst, ▁into, ▁just, ▁keep, ▁kind, ▁know, ▁last, ▁lead, ▁lear, ▁left, ▁life, ▁like, ▁list, ▁long, ▁look, ▁love, ▁made, ▁make, ▁many, ▁mark, ▁mean, ▁miss, ▁more, ▁most, ▁move, ▁much, ▁must, ▁name, ▁need, ▁next, ▁okay, ▁once, ▁only, ▁open, ▁over, ▁part, ▁pass, ▁peop, ▁pers, ▁plan, ▁play, ▁poss, ...]  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [ately, ather, ating, ation, ative, atter, ature, ction, erest, ether, ident, ility, iness, ision, ities, ition, ittle, ments, other, ought, ready, thing, ually, uring, ution, velop]  \n",
       "10                                                                                                                                                   [▁about, ▁after, ▁again, ▁allow, ▁being, ▁belie, ▁build, ▁child, ▁claim, ▁clear, ▁could, ▁count, ▁court, ▁creat, ▁doesn, ▁doing, ▁eight, ▁every, ▁exper, ▁first, ▁found, ▁gener, ▁going, ▁gonna, ▁great, ▁house, ▁inter, ▁light, ▁maybe, ▁means, ▁might, ▁money, ▁never, ▁order, ▁other, ▁place, ▁point, ▁power, ▁produ, ▁quest, ▁right, ▁small, ▁somet, ▁stand, ▁start, ▁state, ▁still, ▁thank, ▁their, ▁there, ▁these, ▁thing, ▁think, ▁those, ▁thous, ▁three, ▁today, ▁trans, ▁under, ▁using, ▁video, ▁watch, ▁water, ▁where, ▁which, ▁while, ▁whole, ▁world, ▁would, ▁years]  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [ations, ention, ertain, ically, idence]  \n",
       "12                                                                                                                                                                                                                                                                                                                                         [▁<UNK>▁, ▁always, ▁americ, ▁around, ▁before, ▁better, ▁called, ▁change, ▁commun, ▁compan, ▁comple, ▁consid, ▁contin, ▁course, ▁enough, ▁experi, ▁family, ▁follow, ▁govern, ▁happen, ▁having, ▁import, ▁includ, ▁inform, ▁little, ▁making, ▁number, ▁partic, ▁people, ▁person, ▁pretty, ▁really, ▁reason, ▁record, ▁saying, ▁school, ▁second, ▁should, ▁system, ▁things, ▁though, ▁trying]  \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [ational]  \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [▁already, ▁another, ▁because, ▁believe, ▁between, ▁certain, ▁develop, ▁differe, ▁example, ▁getting, ▁hundred, ▁looking, ▁problem, ▁process, ▁support, ▁thought, ▁through, ▁underst, ▁whether, ▁without, ▁working]  \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [▁actually, ▁anything, ▁business, ▁interest, ▁probably, ▁question, ▁thousand, ▁together]  \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [▁different, ▁important, ▁something]  \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [▁everything, ▁government, ▁understand]  \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [▁information]  \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vocabulary(\"Parakeet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>nchars</th>\n",
       "      <th>start</th>\n",
       "      <th>#tokens</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>▁</td>\n",
       "      <td>1</td>\n",
       "      <td>[▁]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#</td>\n",
       "      <td>40</td>\n",
       "      <td>[', -, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, à, â, ç, è, é, ê, ë, î, ï, ô, ù, û]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>▁</td>\n",
       "      <td>26</td>\n",
       "      <td>[▁a, ▁b, ▁c, ▁d, ▁e, ▁f, ▁g, ▁h, ▁i, ▁j, ▁k, ▁l, ▁m, ▁n, ▁o, ▁p, ▁r, ▁s, ▁t, ▁u, ▁v, ▁w, ▁y, ▁z, ▁à, ▁é]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>#</td>\n",
       "      <td>125</td>\n",
       "      <td>[ab, ac, ad, ag, ai, al, am, an, ap, ar, as, at, au, av, ay, aî, be, ce, ch, ci, ct, cu, cé, de, di, du, dé, el, em, en, er, es, eu, ez, ff, fi, fs, ge, gn, gu, ic, ie, if, ig, il, im, in, ir, is, it, je, la, le, li, ll, lo, ls, là, lè, lé, mb, me, mi, mp, mé, ne, ob, oc, og, oi, ol, om, on, op, or, os, ot, ou, oy, pe, ph, pp, pr, pt, qu, ra, re, ri, ré, se, si, ta, te, th, ti, tr, ts, tt, tu, té, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>▁</td>\n",
       "      <td>110</td>\n",
       "      <td>[▁ab, ▁ac, ▁ad, ▁ag, ▁ah, ▁ai, ▁al, ▁am, ▁an, ▁ap, ▁ar, ▁as, ▁au, ▁av, ▁ba, ▁be, ▁bi, ▁bl, ▁br, ▁ca, ▁ce, ▁ch, ▁cl, ▁co, ▁cr, ▁cô, ▁de, ▁di, ▁dr, ▁du, ▁dé, ▁en, ▁es, ▁et, ▁eu, ▁ex, ▁fa, ▁fi, ▁fr, ▁gr, ▁he, ▁hm, ▁id, ▁il, ▁im, ▁in, ▁je, ▁ju, ▁la, ▁le, ▁li, ▁lu, ▁là, ▁ma, ▁me, ▁mi, ▁mo, ▁mé, ▁ne, ▁ni, ▁no, ▁né, ▁ob, ▁oc, ▁oh, ▁on, ▁op, ▁or, ▁ou, ▁où, ▁pa, ▁pe, ▁ph, ▁pi, ▁pl, ▁po, ▁pr, ▁qu, ▁ra, ▁re, ▁ri, ▁ré, ▁sa, ▁sc, ▁se, ▁si, ▁so, ▁sp, ▁st, ▁su, ▁sy, ▁sé, ▁sû, ▁ta, ▁te, ▁th, ▁ti, ▁tr, ▁tu, ▁té, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>#</td>\n",
       "      <td>151</td>\n",
       "      <td>[ace, act, age, agn, ain, ais, ait, ale, all, ame, anc, and, ang, ans, ant, ard, ari, ass, ati, ats, aut, aux, ble, bre, cer, ces, che, ché, cin, cip, cle, cti, cul, dem, der, des, dre, end, ens, ent, ers, ert, euf, eur, eux, gne, hui, ici, ien, ier, ies, ign, ill, ine, ing, ins, ion, ise, iss, ist, itu, ité, jet, ler, les, lic, lig, lle, lui, mer, mes, min, mis, mps, ner, nes, oin, oir, ois, oit, ole, olu, omb, omp, ond, one, ong, onn, ons, ont, ord, ore, orm, ors, ort, ose, oup, our, ous, out, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>▁</td>\n",
       "      <td>166</td>\n",
       "      <td>[▁acc, ▁aff, ▁ain, ▁all, ▁ann, ▁ans, ▁app, ▁arr, ▁ass, ▁att, ▁auc, ▁aus, ▁aut, ▁aux, ▁ave, ▁bah, ▁ben, ▁bes, ▁bon, ▁bou, ▁cap, ▁car, ▁cas, ▁ces, ▁cet, ▁cha, ▁che, ▁cin, ▁cli, ▁com, ▁con, ▁cor, ▁cou, ▁cré, ▁dem, ▁der, ▁des, ▁dev, ▁dis, ▁dit, ▁dix, ▁don, ▁dou, ▁déb, ▁déc, ▁déf, ▁déj, ▁dém, ▁dép, ▁eff, ▁emp, ▁enc, ▁enf, ▁ens, ▁ent, ▁env, ▁esp, ▁ess, ▁est, ▁euh, ▁eur, ▁eux, ▁exe, ▁exp, ▁fam, ▁fem, ▁fer, ▁fil, ▁fin, ▁fon, ▁for, ▁fut, ▁gra, ▁gén, ▁hab, ▁hom, ▁hum, ▁ici, ▁ils, ▁imp, ▁inc, ▁inf, ▁ins, ▁jam, ▁jou, ▁jus, ▁les, ▁lui, ▁mad, ▁mal, ▁man, ▁mar, ▁mer, ▁mes, ▁mil, ▁min, ▁mis, ▁moi, ▁mom, ▁mon, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>#</td>\n",
       "      <td>88</td>\n",
       "      <td>[able, ages, agne, aine, ains, aire, aiss, ales, alis, ance, ande, ange, anis, ante, ants, arde, asse, atre, cher, ches, coup, dent, duit, elle, ence, ends, enir, enne, ense, ente, ents, eure, eurs, euse, iens, ille, ingt, ions, ique, iste, iter, ités, ième, ière, jour, lier, lles, lopp, mble, ment, mple, nent, oins, oire, olog, onne, onom, orte, orti, oses, otre, ours, ouve, près, puis, quer, ques, quoi, rais, rait, rent, ress, sion, tant, tion, tout, tres, ttre, uite, ures, veau, vent, vers, voir, vous, ères, êtes, être]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>▁</td>\n",
       "      <td>134</td>\n",
       "      <td>[▁alle, ▁appr, ▁arri, ▁avec, ▁avez, ▁beau, ▁bien, ▁cela, ▁cent, ▁cert, ▁ceux, ▁char, ▁cher, ▁chez, ▁cinq, ▁comb, ▁comm, ▁comp, ▁conc, ▁conf, ▁conn, ▁cons, ▁cont, ▁coup, ▁cour, ▁côté, ▁dans, ▁dern, ▁deux, ▁dieu, ▁diff, ▁dire, ▁doit, ▁donc, ▁donn, ▁dont, ▁déjà, ▁effe, ▁elle, ▁ense, ▁fais, ▁fait, ▁faut, ▁fois, ▁fond, ▁form, ▁fran, ▁gens, ▁gros, ▁hein, ▁hist, ▁indi, ▁inté, ▁jour, ▁leur, ▁long, ▁lors, ▁main, ▁mais, ▁mois, ▁mont, ▁mort, ▁même, ▁neuf, ▁nous, ▁parf, ▁part, ▁pass, ▁pays, ▁pens, ▁pers, ▁peti, ▁peut, ▁peux, ▁plan, ▁plus, ▁plut, ▁poin, ▁poli, ▁pour, ▁pouv, ▁pres, ▁prin, ▁pris, ▁prob, ▁prof, ▁prop, ▁prés, ▁puis, ▁quar, ▁quel, ▁ques, ▁quoi, ▁rais, ▁rapp, ▁repr, ▁retr, ▁rien, ▁sain, ▁sais, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>#</td>\n",
       "      <td>25</td>\n",
       "      <td>[ables, aient, aines, aires, alité, ances, anger, ation, ature, ction, endre, eures, ieurs, ilité, iques, jourd, jours, lique, ments, ouver, ouvez, sible, sieur, tions, tique]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>▁</td>\n",
       "      <td>65</td>\n",
       "      <td>[▁ainsi, ▁aller, ▁allez, ▁alors, ▁après, ▁assez, ▁aussi, ▁autre, ▁avais, ▁avait, ▁avant, ▁avoir, ▁avons, ▁bonne, ▁celui, ▁cette, ▁chose, ▁cinqu, ▁comme, ▁compr, ▁contr, ▁crois, ▁droit, ▁elles, ▁enfin, ▁entre, ▁europ, ▁faire, ▁franç, ▁grand, ▁heure, ▁homme, ▁inter, ▁jours, ▁jusqu, ▁juste, ▁leurs, ▁mieux, ▁mille, ▁milli, ▁moins, ▁monde, ▁notre, ▁nouve, ▁ouais, ▁parce, ▁parle, ▁parti, ▁passe, ▁pense, ▁perme, ▁petit, ▁place, ▁point, ▁premi, ▁quand, ▁temps, ▁toute, ▁trans, ▁trois, ▁vidéo, ▁vingt, ▁voilà, ▁votre, ▁était]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>#</td>\n",
       "      <td>11</td>\n",
       "      <td>[amment, ations, atique, endant, lement, lleurs, nement, sition, tement, velopp, vement]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>▁</td>\n",
       "      <td>41</td>\n",
       "      <td>[▁&lt;UNK&gt;▁, ▁accord, ▁années, ▁autres, ▁besoin, ▁chaque, ▁choses, ▁commen, ▁commun, ▁compte, ▁contin, ▁contre, ▁demand, ▁depuis, ▁différ, ▁donner, ▁encore, ▁europé, ▁france, ▁grande, ▁import, ▁jamais, ▁mettre, ▁moment, ▁niveau, ▁nombre, ▁parler, ▁partie, ▁passer, ▁petite, ▁pouvez, ▁quatre, ▁regard, ▁répond, ▁savoir, ▁simple, ▁toutes, ▁travai, ▁trente, ▁trouve, ▁utilis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>#</td>\n",
       "      <td>2</td>\n",
       "      <td>[alement, llement]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>▁</td>\n",
       "      <td>19</td>\n",
       "      <td>[▁appelle, ▁aujourd, ▁comment, ▁enfants, ▁ensuite, ▁entrepr, ▁exemple, ▁mainten, ▁pendant, ▁personn, ▁pouvoir, ▁premier, ▁prendre, ▁présent, ▁quelque, ▁souvent, ▁surtout, ▁travail, ▁étaient]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>▁</td>\n",
       "      <td>14</td>\n",
       "      <td>[▁ailleurs, ▁beaucoup, ▁fonction, ▁français, ▁histoire, ▁monsieur, ▁personne, ▁pourquoi, ▁première, ▁quelques, ▁question, ▁soixante, ▁toujours, ▁vraiment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>▁</td>\n",
       "      <td>5</td>\n",
       "      <td>[▁cinquante, ▁important, ▁personnes, ▁plusieurs, ▁également]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>▁</td>\n",
       "      <td>1</td>\n",
       "      <td>[▁maintenant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td></td>\n",
       "      <td>1024</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nchars start  #tokens  \\\n",
       "0       0  ▁        1      \n",
       "1       1  #       40      \n",
       "2       1  ▁       26      \n",
       "3       2  #      125      \n",
       "4       2  ▁      110      \n",
       "5       3  #      151      \n",
       "6       3  ▁      166      \n",
       "7       4  #       88      \n",
       "8       4  ▁      134      \n",
       "9       5  #       25      \n",
       "10      5  ▁       65      \n",
       "11      6  #       11      \n",
       "12      6  ▁       41      \n",
       "13      7  #        2      \n",
       "14      7  ▁       19      \n",
       "15      8  ▁       14      \n",
       "16      9  ▁        5      \n",
       "17     10  ▁        1      \n",
       "18  TOTAL        1024      \n",
       "\n",
       "   tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [▁]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [', -, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, à, â, ç, è, é, ê, ë, î, ï, ô, ù, û]  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [▁a, ▁b, ▁c, ▁d, ▁e, ▁f, ▁g, ▁h, ▁i, ▁j, ▁k, ▁l, ▁m, ▁n, ▁o, ▁p, ▁r, ▁s, ▁t, ▁u, ▁v, ▁w, ▁y, ▁z, ▁à, ▁é]  \n",
       "3                                                                                                                                                                                                                                                                                                               [ab, ac, ad, ag, ai, al, am, an, ap, ar, as, at, au, av, ay, aî, be, ce, ch, ci, ct, cu, cé, de, di, du, dé, el, em, en, er, es, eu, ez, ff, fi, fs, ge, gn, gu, ic, ie, if, ig, il, im, in, ir, is, it, je, la, le, li, ll, lo, ls, là, lè, lé, mb, me, mi, mp, mé, ne, ob, oc, og, oi, ol, om, on, op, or, os, ot, ou, oy, pe, ph, pp, pr, pt, qu, ra, re, ri, ré, se, si, ta, te, th, ti, tr, ts, tt, tu, té, ...]  \n",
       "4                                                                                                                                                                                                           [▁ab, ▁ac, ▁ad, ▁ag, ▁ah, ▁ai, ▁al, ▁am, ▁an, ▁ap, ▁ar, ▁as, ▁au, ▁av, ▁ba, ▁be, ▁bi, ▁bl, ▁br, ▁ca, ▁ce, ▁ch, ▁cl, ▁co, ▁cr, ▁cô, ▁de, ▁di, ▁dr, ▁du, ▁dé, ▁en, ▁es, ▁et, ▁eu, ▁ex, ▁fa, ▁fi, ▁fr, ▁gr, ▁he, ▁hm, ▁id, ▁il, ▁im, ▁in, ▁je, ▁ju, ▁la, ▁le, ▁li, ▁lu, ▁là, ▁ma, ▁me, ▁mi, ▁mo, ▁mé, ▁ne, ▁ni, ▁no, ▁né, ▁ob, ▁oc, ▁oh, ▁on, ▁op, ▁or, ▁ou, ▁où, ▁pa, ▁pe, ▁ph, ▁pi, ▁pl, ▁po, ▁pr, ▁qu, ▁ra, ▁re, ▁ri, ▁ré, ▁sa, ▁sc, ▁se, ▁si, ▁so, ▁sp, ▁st, ▁su, ▁sy, ▁sé, ▁sû, ▁ta, ▁te, ▁th, ▁ti, ▁tr, ▁tu, ▁té, ...]  \n",
       "5                                                                                                                                                                                                           [ace, act, age, agn, ain, ais, ait, ale, all, ame, anc, and, ang, ans, ant, ard, ari, ass, ati, ats, aut, aux, ble, bre, cer, ces, che, ché, cin, cip, cle, cti, cul, dem, der, des, dre, end, ens, ent, ers, ert, euf, eur, eux, gne, hui, ici, ien, ier, ies, ign, ill, ine, ing, ins, ion, ise, iss, ist, itu, ité, jet, ler, les, lic, lig, lle, lui, mer, mes, min, mis, mps, ner, nes, oin, oir, ois, oit, ole, olu, omb, omp, ond, one, ong, onn, ons, ont, ord, ore, orm, ors, ort, ose, oup, our, ous, out, ...]  \n",
       "6                                                                                                       [▁acc, ▁aff, ▁ain, ▁all, ▁ann, ▁ans, ▁app, ▁arr, ▁ass, ▁att, ▁auc, ▁aus, ▁aut, ▁aux, ▁ave, ▁bah, ▁ben, ▁bes, ▁bon, ▁bou, ▁cap, ▁car, ▁cas, ▁ces, ▁cet, ▁cha, ▁che, ▁cin, ▁cli, ▁com, ▁con, ▁cor, ▁cou, ▁cré, ▁dem, ▁der, ▁des, ▁dev, ▁dis, ▁dit, ▁dix, ▁don, ▁dou, ▁déb, ▁déc, ▁déf, ▁déj, ▁dém, ▁dép, ▁eff, ▁emp, ▁enc, ▁enf, ▁ens, ▁ent, ▁env, ▁esp, ▁ess, ▁est, ▁euh, ▁eur, ▁eux, ▁exe, ▁exp, ▁fam, ▁fem, ▁fer, ▁fil, ▁fin, ▁fon, ▁for, ▁fut, ▁gra, ▁gén, ▁hab, ▁hom, ▁hum, ▁ici, ▁ils, ▁imp, ▁inc, ▁inf, ▁ins, ▁jam, ▁jou, ▁jus, ▁les, ▁lui, ▁mad, ▁mal, ▁man, ▁mar, ▁mer, ▁mes, ▁mil, ▁min, ▁mis, ▁moi, ▁mom, ▁mon, ...]  \n",
       "7                                                                                                                                                                                    [able, ages, agne, aine, ains, aire, aiss, ales, alis, ance, ande, ange, anis, ante, ants, arde, asse, atre, cher, ches, coup, dent, duit, elle, ence, ends, enir, enne, ense, ente, ents, eure, eurs, euse, iens, ille, ingt, ions, ique, iste, iter, ités, ième, ière, jour, lier, lles, lopp, mble, ment, mple, nent, oins, oire, olog, onne, onom, orte, orti, oses, otre, ours, ouve, près, puis, quer, ques, quoi, rais, rait, rent, ress, sion, tant, tion, tout, tres, ttre, uite, ures, veau, vent, vers, voir, vous, ères, êtes, être]  \n",
       "8   [▁alle, ▁appr, ▁arri, ▁avec, ▁avez, ▁beau, ▁bien, ▁cela, ▁cent, ▁cert, ▁ceux, ▁char, ▁cher, ▁chez, ▁cinq, ▁comb, ▁comm, ▁comp, ▁conc, ▁conf, ▁conn, ▁cons, ▁cont, ▁coup, ▁cour, ▁côté, ▁dans, ▁dern, ▁deux, ▁dieu, ▁diff, ▁dire, ▁doit, ▁donc, ▁donn, ▁dont, ▁déjà, ▁effe, ▁elle, ▁ense, ▁fais, ▁fait, ▁faut, ▁fois, ▁fond, ▁form, ▁fran, ▁gens, ▁gros, ▁hein, ▁hist, ▁indi, ▁inté, ▁jour, ▁leur, ▁long, ▁lors, ▁main, ▁mais, ▁mois, ▁mont, ▁mort, ▁même, ▁neuf, ▁nous, ▁parf, ▁part, ▁pass, ▁pays, ▁pens, ▁pers, ▁peti, ▁peut, ▁peux, ▁plan, ▁plus, ▁plut, ▁poin, ▁poli, ▁pour, ▁pouv, ▁pres, ▁prin, ▁pris, ▁prob, ▁prof, ▁prop, ▁prés, ▁puis, ▁quar, ▁quel, ▁ques, ▁quoi, ▁rais, ▁rapp, ▁repr, ▁retr, ▁rien, ▁sain, ▁sais, ...]  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [ables, aient, aines, aires, alité, ances, anger, ation, ature, ction, endre, eures, ieurs, ilité, iques, jourd, jours, lique, ments, ouver, ouvez, sible, sieur, tions, tique]  \n",
       "10                                                                                                                                                                                           [▁ainsi, ▁aller, ▁allez, ▁alors, ▁après, ▁assez, ▁aussi, ▁autre, ▁avais, ▁avait, ▁avant, ▁avoir, ▁avons, ▁bonne, ▁celui, ▁cette, ▁chose, ▁cinqu, ▁comme, ▁compr, ▁contr, ▁crois, ▁droit, ▁elles, ▁enfin, ▁entre, ▁europ, ▁faire, ▁franç, ▁grand, ▁heure, ▁homme, ▁inter, ▁jours, ▁jusqu, ▁juste, ▁leurs, ▁mieux, ▁mille, ▁milli, ▁moins, ▁monde, ▁notre, ▁nouve, ▁ouais, ▁parce, ▁parle, ▁parti, ▁passe, ▁pense, ▁perme, ▁petit, ▁place, ▁point, ▁premi, ▁quand, ▁temps, ▁toute, ▁trans, ▁trois, ▁vidéo, ▁vingt, ▁voilà, ▁votre, ▁était]  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [amment, ations, atique, endant, lement, lleurs, nement, sition, tement, velopp, vement]  \n",
       "12                                                                                                                                                                                                                                                                                                                                                  [▁<UNK>▁, ▁accord, ▁années, ▁autres, ▁besoin, ▁chaque, ▁choses, ▁commen, ▁commun, ▁compte, ▁contin, ▁contre, ▁demand, ▁depuis, ▁différ, ▁donner, ▁encore, ▁europé, ▁france, ▁grande, ▁import, ▁jamais, ▁mettre, ▁moment, ▁niveau, ▁nombre, ▁parler, ▁partie, ▁passer, ▁petite, ▁pouvez, ▁quatre, ▁regard, ▁répond, ▁savoir, ▁simple, ▁toutes, ▁travai, ▁trente, ▁trouve, ▁utilis]  \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [alement, llement]  \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [▁appelle, ▁aujourd, ▁comment, ▁enfants, ▁ensuite, ▁entrepr, ▁exemple, ▁mainten, ▁pendant, ▁personn, ▁pouvoir, ▁premier, ▁prendre, ▁présent, ▁quelque, ▁souvent, ▁surtout, ▁travail, ▁étaient]  \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [▁ailleurs, ▁beaucoup, ▁fonction, ▁français, ▁histoire, ▁monsieur, ▁personne, ▁pourquoi, ▁première, ▁quelques, ▁question, ▁soixante, ▁toujours, ▁vraiment]  \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [▁cinquante, ▁important, ▁personnes, ▁plusieurs, ▁également]  \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [▁maintenant]  \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vocabulary(\"Perruche\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ☪ Understand how tokenizers work with Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>size</th>\n",
       "      <th>#tokens</th>\n",
       "      <th>fert.</th>\n",
       "      <th>encoded tokens</th>\n",
       "      <th>round-trip</th>\n",
       "      <th>decoded text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jais</td>\n",
       "      <td>64k</td>\n",
       "      <td>13</td>\n",
       "      <td>3.2</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏‎�┃‏‎�┃‏┃،┃كيف┃ح┃الك‎Pierre┃‏‎-┃‏‎Jean┃‏┃مرحبا┃ً‎&lt;BOS&gt;</td>\n",
       "      <td>✅</td>\n",
       "      <td>‎&lt;EOS&gt;‏، كيف حالك؟‎Jean-Pierre ‏ مرحباً ‎&lt;BOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aya</td>\n",
       "      <td>255k</td>\n",
       "      <td>15</td>\n",
       "      <td>3.8</td>\n",
       "      <td>‎&lt;EOS&gt;┃‏حال┃ك┃؟‎▁┃‏ك┃يف‎▁┃‏┃،‎Pierre┃‏‎-┃‏‎▁Jean┃‏┃مر┃ح┃با┃ً‎&lt;BOS&gt;</td>\n",
       "      <td>✅</td>\n",
       "      <td>‎&lt;EOS&gt;‏، كيف حالك؟‎Jean-Pierre ‏مرحباً ‎&lt;BOS&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tokenizer size   #tokens  fert.  \\\n",
       "0  Jais       64k  13       3.2     \n",
       "1   Aya      255k  15       3.8     \n",
       "\n",
       "  encoded tokens                                                       \\\n",
       "0      ‎<EOS>┃‏‎�┃‏‎�┃‏┃،┃كيف┃ح┃الك‎Pierre┃‏‎-┃‏‎Jean┃‏┃مرحبا┃ً‎<BOS>   \n",
       "1  ‎<EOS>┃‏حال┃ك┃؟‎▁┃‏ك┃يف‎▁┃‏┃،‎Pierre┃‏‎-┃‏‎▁Jean┃‏┃مر┃ح┃با┃ً‎<BOS>   \n",
       "\n",
       "  round-trip decoded text                                     \n",
       "0  ✅          ‎<EOS>‏، كيف حالك؟‎Jean-Pierre ‏ مرحباً ‎<BOS>  \n",
       "1  ✅           ‎<EOS>‏، كيف حالك؟‎Jean-Pierre ‏مرحباً ‎<BOS>  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"مرحباً Jean-Pierre، كيف حالك؟\"\n",
    "\n",
    "test_tokenizers_on_text(input, [\"Jais\", \"Aya\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
