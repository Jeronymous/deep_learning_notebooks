{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” Tokenizers (in Deep Learning)\n",
    "\n",
    "## ğŸ“– Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer maps `string` $\\rightleftharpoons$ `list of tokens`.\n",
    "* `encode`(\"string\") $\\mapsto$ [\"list\", \"of\", \"tokens\"]\n",
    "* `decode`([\"list\", \"of\", \"tokens\"]) $\\mapsto$ \"string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DeepLearning/NLP, a token is a unit of text (sequence of characters or bytes) that is often characterized by:\n",
    "* an <u>index in the vocabulary</u> of tokens,\n",
    "* a string representer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index is used to interpret inputs and/or outputs of a neural network.\n",
    "<br> It is a positive integer that is often used to index vectorial embeddings.\n",
    "<br> The *encoding* part is used when the neural net is fed with text data (ex: Language Models).\n",
    "<br> The *decoding* part is used when the neural net outputs text data (ex: Text Generation with LM, Automatic Speech Recognition, Image Caption Generation, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When extracted from a string, a token can also be characterized the <u>positions in the original string</u> (start & end). <br>\n",
    "This is needed for application like extractive question answering, where we have to track the position of the answer in the original text to extract it. <br>\n",
    "(Caution with multi-bytes characters: position in bytes is not the same as position in characters.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good tokenizer can compress a string into a small number of tokens,\n",
    "while keeping a reasonable vocabulary size (number of possible unique tokens). <br>\n",
    "A popular measure of this efficiency is the <u>fertility</u> of the tokenizer, defined as the average number of tokens per word:\n",
    "$$\\text{fertility} = \\frac{\\text{number of tokens}}{\\text{number of words}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decomposition of a string into tokens can be illustrated in this small code, where tokens can be words/characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CharSplitter ==\n",
      "-> ['M', 'a', 'i', 's', ',', ' ', 'm', 'a', 'i', 's', 'â€¦', ' ', 'v', 'a', 's', ' ', 't', \"'\", 'e', 'n', ' ', 'l', 'Ã ', '-', 'b', 'a', 's', ' ', '!']\n",
      "-> Mais, maisâ€¦ vas t'en lÃ -bas !\n",
      "\n",
      "== WordSplitter ==\n",
      "-> ['Mais,', 'â–maisâ€¦', 'â–vas', \"â–t'en\", 'â–lÃ -bas', 'â–!']\n",
      "-> Mais, maisâ€¦ vas t'en lÃ -bas !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class CharSplitter:\n",
    "\n",
    "    def split(self, text: str) -> list:\n",
    "        return list(text)\n",
    "\n",
    "    def join(self, tokens: list) -> str:\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "class WordSplitter:\n",
    "\n",
    "    _SPACE = \"â–\"\n",
    "\n",
    "    def split(self, text: str) -> list:\n",
    "        words = text.split(\" \")\n",
    "        return ([words[0]] + [self._SPACE+w for w in words[1:]]) if words else []\n",
    "        \n",
    "    def join(self, tokens: list) -> str:\n",
    "        return \"\".join(tokens).replace(self._SPACE, \" \")\n",
    "        \n",
    "input = \"Mais, maisâ€¦ vas t'en lÃ -bas !\"\n",
    "\n",
    "for tokenizer in [\n",
    "    CharSplitter(),\n",
    "    WordSplitter(),\n",
    "    ]:\n",
    "\n",
    "    encoded = tokenizer.split(input)\n",
    "\n",
    "    # Round-trip test\n",
    "    encoded_decoded = tokenizer.join(encoded)\n",
    "    assert encoded_decoded == input\n",
    "    \n",
    "    print(f\"== {tokenizer.__class__.__name__} ==\\n-> {encoded}\\n-> {encoded_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was to show the string decomposition principle.\n",
    "\n",
    "In reality, tokenizers are more complex when they are used with a neural network model, because of the vocabulary that has a **fixed size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    _SOS = \"<start>\"\n",
    "    _EOS = \"<end>\"\n",
    "    _UNK = \"<unk>\"\n",
    "    vocabulary = [_UNK, _SOS, _EOS]\n",
    "   \n",
    "    def encode(self, text: str) -> list:\n",
    "        # Dec\n",
    "        tokens_str = self.split(text)\n",
    "        tokens_idx = [\n",
    "            self.vocabulary.index(t) if t in self.vocabulary\n",
    "            else self.vocabulary.index(self._UNK)\n",
    "            for t in tokens_str\n",
    "        ]\n",
    "        return tokens_idx\n",
    "        \n",
    "    def encode_str(self, text: str) -> list:\n",
    "        return [self.vocabulary[idx] for idx in self.encode(text)]\n",
    "\n",
    "class CharTokenizer(Tokenizer, CharSplitter):\n",
    "\n",
    "    def __init__(self, vocabulary: list = [chr(i) for i in range(128)]):\n",
    "        self.vocabulary += vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mais, maisâ€¦ vas t'en lÃ -bas !\n",
      "-> [80, 100, 108, 118, 47, 35, 112, 100, 108, 118, 0, 35, 121, 100, 118, 35, 119, 42, 104, 113, 35, 111, 0, 48, 101, 100, 118, 35, 36]\n",
      "-> ['M', 'a', 'i', 's', ',', ' ', 'm', 'a', 'i', 's', '<unk>', ' ', 'v', 'a', 's', ' ', 't', \"'\", 'e', 'n', ' ', 'l', '<unk>', '-', 'b', 'a', 's', ' ', '!']\n",
      "-> Mais, mais<unk> vas t'en l<unk>-bas !\n"
     ]
    }
   ],
   "source": [
    "input = \"Mais, maisâ€¦ vas t'en lÃ -bas !\"\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "encoded = tokenizer.encode(input)\n",
    "encoded_str = tokenizer.encode_str(input)\n",
    "encoded_decoded = tokenizer.join(encoded_str)\n",
    "print(f\"{input}\\n-> {encoded}\\n-> {encoded_str}\\n-> {encoded_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Install common librairies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular python libraries for tokenizers are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular tokenizer libraries\n",
    "!pip install tokenizers>=0.20,<0.21 # last version is buggy\n",
    "!pip install tiktoken #==0.8.0\n",
    "!pip install sentencepiece #==0.2.0\n",
    "\n",
    "# Librairies for neural networks that include tokenizers (usually wrap other lower-level tokenization libraries)\n",
    "!pip install transformers #==4.46.3\n",
    "# !pip install nemo #==6.0.3\n",
    "!pip install git+https://github.com/linagora-labs/NeMo.git pytorch_lightning==2.4.0 lhotse==1.28.0\n",
    "#!python -m pip install git+https://github.com/linagora-labs/NeMo.git@{main}#egg=nemo_toolkit[asr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip index versions tiktoken 2> /dev/null\n",
    "!pip index versions tokenizers 2> /dev/null\n",
    "!pip index versions sentencepiece 2> /dev/null\n",
    "\n",
    "# !pip index versions transformers 2> /dev/null\n",
    "# !pip index versions nemo 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also useful for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘¨â€ğŸ’» Helpers for an inspection of several tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of interesting tokenizers, identified either by their tiktoken name or [Hugging Face repository name](https://huggingface.co/docs/huggingface_hub/en/guides/repository)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizers = {\n",
    "    \n",
    "    # LLM - TikToken\n",
    "    \"GPT 3.5\": \"gpt-3.5-turbo\",\n",
    "    \"GPT 4\": \"gpt-4\",\n",
    "    \n",
    "    # LLM - Hugging Face / transformers\n",
    "    \"Gemma\": \"google/gemma-7b\",\n",
    "    \"Qwen\": \"Qwen/Qwen2.5-7B\",\n",
    "    \"Falcon\": \"tiiuae/falcon-7b\",\n",
    "    \"Mistral\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"Llama 2\": \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"Llama 3\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"Croissant\": \"croissantllm/CroissantLLMBase\",\n",
    "    \"Lucie\": \"OpenLLM-France/Lucie-7B\",  # -> https://huggingface.co/OpenLLM-France/Lucie-7B\n",
    "    \"Bloom\": \"bigscience/bloom-7b1\",\n",
    "    \"Olmo 2\": \"allenai/OLMo-2-1124-7B-Instruct\",\n",
    "    \"C4\": \"CohereForAI/c4ai-command-r-plus\",\n",
    "    \"Aya\": \"CohereForAI/aya-expanse-8b\",\n",
    "    \"Jais\": \"inceptionai/jais-adapted-7b-chat\",\n",
    "    \"EuroLLM\": \"utter-project/EuroLLM-9B\",\n",
    "\n",
    "    # ASR - Hugging Face / transformers\n",
    "    \"Whisper\": \"openai/whisper-large-v3\",\n",
    "    # ASR - Hugging Face / nemo\n",
    "    \"Parakeet\": \"nvidia/parakeet-ctc-1.1b\", # same as \"nvidia/parakeet-rnnt-1.1b\",\n",
    "    \"Perruche\": \"/home/jlouradour/projects/Parakeet/tokenizer_spe_bpe_v1024/tokenizer.model\", # /!!!!\\\\\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** some models (like [Llama](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)) might require to fill some agreements form to have access to the model (with your Hugging Face account).<br>\n",
    "In case of problem, you should have an explicit message with the URL to visit (and fill the form) to get access to the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helpers to load and play with tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy # text encoding transformations\n",
    "import pandas as pd\n",
    "\n",
    "def load_tokenizer(name):\n",
    "    \"\"\"\n",
    "    Load a tokenizer by name.\n",
    "    \"\"\"\n",
    "    # Conform name\n",
    "    global my_tokenizers\n",
    "    if name in my_tokenizers:\n",
    "        name = my_tokenizers[name]\n",
    "\n",
    "    # Load with the right library\n",
    "    # - TikToken\n",
    "    if name.lower().startswith(\"gpt\"):\n",
    "        import tiktoken\n",
    "        return tiktoken.encoding_for_model(name.lower())\n",
    "    # - SentencePiece\n",
    "    elif name.endswith(\".model\"):\n",
    "        import sentencepiece as spm\n",
    "        return spm.SentencePieceProcessor(model_file=name)\n",
    "    # - NeMo\n",
    "    elif name.lower().startswith(\"nvidia\"):\n",
    "        import nemo.collections.asr as nemo_asr\n",
    "        import logging\n",
    "        logging.getLogger('nemo_logger').setLevel(logging.ERROR)\n",
    "        if \"ctc\" in name:\n",
    "            nemo_model_class = nemo_asr.models.EncDecCTCModelBPE\n",
    "        elif \"rnn\" in name:\n",
    "            nemo_model_class = nemo_asr.models.EncDecRNNTBPEModel\n",
    "        else:\n",
    "            raise NotImplementedError(f\"NeMo model '{name}' not supported\")\n",
    "        model = nemo_model_class.from_pretrained(name)\n",
    "        return model.tokenizer.tokenizer\n",
    "    # - Transformers\n",
    "    else:\n",
    "        import transformers\n",
    "        try:\n",
    "            return transformers.AutoTokenizer.from_pretrained(name, trust_remote_code=True)\n",
    "        except Exception as err:\n",
    "            # Report the name in case of error\n",
    "            raise RuntimeError(f\"Could not load tokenizer '{name}': {err}\") from err\n",
    "\n",
    "def load_tokenizer_with_cache(name):\n",
    "    \"\"\"\n",
    "    Load a tokenizer by name, with cache.\n",
    "    \"\"\"\n",
    "    global _loaded_tokenizers\n",
    "    if name not in _loaded_tokenizers:\n",
    "        _loaded_tokenizers[name] = load_tokenizer(name)\n",
    "    return _loaded_tokenizers[name] \n",
    "\n",
    "if \"_loaded_tokenizers\" not in globals():\n",
    "    _loaded_tokenizers = {}\n",
    "\n",
    "\n",
    "def encode_decode(tokenizer, text, add_special_tokens=True, use_internal_tokens=False):\n",
    "    \"\"\"\n",
    "    Round-trip testing:\n",
    "    Encode an input text into a list of tokens, and decode the tokens back to a string.\n",
    "    \"\"\"\n",
    "    if isinstance(tokenizer, str):\n",
    "        tokenizer = load_tokenizer_with_cache(tokenizer)\n",
    "\n",
    "    if \"encode_batch\" in dir(tokenizer):\n",
    "        # TikToken\n",
    "        tokens = tokenizer.encode_batch(\n",
    "            [text],\n",
    "            allowed_special=\"all\" if add_special_tokens else set(),\n",
    "            disallowed_special=()\n",
    "        )[0]\n",
    "        if hasattr(tokens, \"ids\"):\n",
    "            tokens = tokens.ids\n",
    "        if use_internal_tokens and hasattr(tokenizer, \"id_to_token\"):\n",
    "            tokens_strings = [tokenizer.id_to_token(t) for t in tokens]\n",
    "        else:\n",
    "            tokens_strings = [tokenizer.decode([t]) for t in tokens]\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "    elif \"SentencePiece\" in type(tokenizer).__name__:\n",
    "        # SentencePiece\n",
    "        tokens = tokenizer.encode(text,\n",
    "            add_bos=add_special_tokens,\n",
    "            add_eos=add_special_tokens,\n",
    "            emit_unk_piece=add_special_tokens,\n",
    "        )\n",
    "        tokens_strings = tokenizer.encode(text,\n",
    "            out_type=str,\n",
    "            emit_unk_piece=add_special_tokens,\n",
    "        )\n",
    "        if add_special_tokens:\n",
    "            assert len(tokens) == len(tokens_strings) + 2\n",
    "            if tokens[0] >= 0:\n",
    "                tokens_strings = [\"<BOS>\"] + tokens_strings\n",
    "            else:\n",
    "                tokens = tokens[1:]\n",
    "            if tokens[-1] >= 0:\n",
    "                tokens_strings += [\"<EOS>\"]\n",
    "            else:\n",
    "                tokens = tokens[:-1]\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "    else:\n",
    "        # transformers / tokenizers\n",
    "        tokenizer.add_eos_token = bool(add_special_tokens)\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=add_special_tokens)\n",
    "        # tokens_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "        tokens_strings = [tokenizer.decode(t, skip_special_tokens=False) for t in tokens]\n",
    "        decoded = tokenizer.decode(tokens, skip_special_tokens=not add_special_tokens)\n",
    "\n",
    "    # Normalize for display\n",
    "    tokens_strings = normalize_for_display(tokens_strings, is_token=True)\n",
    "    norm_decoded = normalize_for_display(decoded)\n",
    "\n",
    "    return tokens, tokens_strings, decoded, norm_decoded\n",
    "\n",
    "\n",
    "def vocabulary_size(tokenizer):\n",
    "    \"\"\"\n",
    "    Get the vocabulary size of a tokenizer.\n",
    "    \"\"\"\n",
    "    if isinstance(tokenizer, str):\n",
    "        tokenizer = load_tokenizer_with_cache(tokenizer)\n",
    "    \n",
    "    N = None\n",
    "    for attr_name in \"n_vocab\", \"vocab_size\":\n",
    "        if attr_name in dir(tokenizer):\n",
    "            N = getattr(tokenizer, attr_name)\n",
    "            if not isinstance(N, int):\n",
    "                assert callable(N)\n",
    "                N = N()\n",
    "            break\n",
    "    if not N:\n",
    "        raise NotImplementedError(f\"Vocabulary not supported for tokenizer '{tokenizer}'\")\n",
    "    return N\n",
    "\n",
    "\n",
    "def get_vocabulary(tokenizer):\n",
    "    \"\"\"\n",
    "    Get the vocabulary of a tokenizer.\n",
    "    \"\"\"\n",
    "    if isinstance(tokenizer, str):\n",
    "        tokenizer = load_tokenizer_with_cache(tokenizer)\n",
    "\n",
    "    N = vocabulary_size(tokenizer)\n",
    "    try:\n",
    "        tokens_str = [tokenizer.decode([t], skip_special_tokens=False) for t in range(N)]\n",
    "    except Exception as err:\n",
    "        tokens_str = [normalize_for_display(tokenizer.decode([t]), is_token=True) for t in range(N)]\n",
    "    if \"id_to_piece\" in dir(tokenizer):\n",
    "        tokens_str_check = [normalize_for_display(tokenizer.id_to_piece(t), is_token=True) for t in range(N)]\n",
    "        for i, (t1, t2) in enumerate(zip(tokens_str, tokens_str_check)):\n",
    "            if t1 != t2:\n",
    "                if not t1.startswith(\"â–\") and t2.startswith(\"â–\"):\n",
    "                    tokens_str[i] = \"â–\" + tokens_str[i]\n",
    "    \n",
    "    tokens_str = [normalize_for_display(tok, is_token=True) for tok in tokens_str]\n",
    "    return tokens_str\n",
    "\n",
    "\n",
    "def sorted_vocabulary(vocab):\n",
    "    if isinstance(vocab, str):\n",
    "        vocab = get_vocabulary(vocab)\n",
    "    tokens_by_length = {}\n",
    "    for token in sorted(vocab):\n",
    "        nchars = len(token.lstrip(\"â–\"))\n",
    "        sow = token.startswith(\"â–\")\n",
    "        key = (nchars, sow)\n",
    "        if key not in tokens_by_length:\n",
    "            tokens_by_length[key] = []\n",
    "        tokens_by_length[key].append(token)\n",
    "\n",
    "    data = []\n",
    "    for (nchars, sow) in sorted(tokens_by_length):\n",
    "        tokens = sorted(tokens_by_length[(nchars, sow)])\n",
    "        data.append({\n",
    "            \"nchars\": nchars,\n",
    "            \"start\": \"â–\" if sow else \"#\",\n",
    "            \"#tokens\": len(tokens),\n",
    "            \"tokens\": tokens # normalize_for_display(tokens, is_token=True),\n",
    "        })\n",
    "\n",
    "    data.append({\n",
    "        \"nchars\": \"TOTAL\",\n",
    "        \"start\": \"\",\n",
    "        \"#tokens\": sum(d[\"#tokens\"] for d in data),\n",
    "        \"tokens\": \"\",\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Will be defined later\n",
    "if \"normalize_for_display\" not in globals():\n",
    "    def normalize_for_display(text, *kargs, **kwargs):\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some helpers to display things nicely. <br>\n",
    "There is a special treatment for string including both Arabic and latin characters (so that characters are displayed in the right order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â˜ª Test: Fix of display for text with Arabic and code-switching â˜ª\n",
      "------------------------------------------------------------------\n",
      "â¡ï¸ Original string\n",
      "Ù…Ø±Ø­Ø¨Ø§Ù‹ Jean-PierreØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\n",
      "â¡ï¸ String for display\n",
      "â€ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸâ€Jean-Pierre â€Ù…Ø±Ø­Ø¨Ø§Ù‹ \n",
      "â¡ï¸ Word decomposition (of original string)\n",
      "  1- â€Ù…Ø±Ø­Ø¨Ø§Ù‹\n",
      "  2- â€ØŒâ€Jean-Pierre\n",
      "  3- â€ÙƒÙŠÙ\n",
      "  4- â€Ø­Ø§Ù„ÙƒØŸ\n",
      "\n",
      " â€ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸâ€Jean-Pierre\n",
      "\n",
      " â€Jean-Pierre â€Ù…Ø±Ø­Ø¨Ø§Ù‹ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Display</th>\n",
       "      <th>Display as tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ù…Ø±Ø­Ø¨Ø§Ù‹ Jean-PierreØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ</td>\n",
       "      <td>â€ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸâ€Jean-Pierre â€Ù…Ø±Ø­Ø¨Ø§Ù‹</td>\n",
       "      <td>â€ØŒâ”ƒÙƒÙŠÙâ”ƒØ­Ø§Ù„ÙƒØŸâ€Jean-Pierreâ”ƒâ€Ù…Ø±Ø­Ø¨Ø§Ù‹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jean-PierreØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ</td>\n",
       "      <td>â€ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸâ€Jean-Pierre</td>\n",
       "      <td>â€ØŒâ”ƒÙƒÙŠÙâ”ƒØ­Ø§Ù„ÙƒØŸâ€Jean-Pierre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ù…Ø±Ø­Ø¨Ø§Ù‹ Jean-Pierre</td>\n",
       "      <td>â€Jean-Pierre â€Ù…Ø±Ø­Ø¨Ø§Ù‹</td>\n",
       "      <td>â€Jean-Pierreâ”ƒâ€Ù…Ø±Ø­Ø¨Ø§Ù‹</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Original                       Display                             \\\n",
       "0  Ù…Ø±Ø­Ø¨Ø§Ù‹ Jean-PierreØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ  â€ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸâ€Jean-Pierre â€Ù…Ø±Ø­Ø¨Ø§Ù‹    \n",
       "1         Jean-PierreØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ           â€ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸâ€Jean-Pierre   \n",
       "2             Ù…Ø±Ø­Ø¨Ø§Ù‹ Jean-Pierre              â€Jean-Pierre â€Ù…Ø±Ø­Ø¨Ø§Ù‹    \n",
       "\n",
       "  Display as tokens                  \n",
       "0  â€ØŒâ”ƒÙƒÙŠÙâ”ƒØ­Ø§Ù„ÙƒØŸâ€Jean-Pierreâ”ƒâ€Ù…Ø±Ø­Ø¨Ø§Ù‹  \n",
       "1          â€ØŒâ”ƒÙƒÙŠÙâ”ƒØ­Ø§Ù„ÙƒØŸâ€Jean-Pierre  \n",
       "2              â€Jean-Pierreâ”ƒâ€Ù…Ø±Ø­Ø¨Ø§Ù‹  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_for_display(text, is_token=False, fix_arabic=True):\n",
    "    \"\"\"\n",
    "    Normalize token/text for display.\n",
    "    \"\"\"\n",
    "    if isinstance(text, list):\n",
    "        \n",
    "        if is_token:\n",
    "            token_list = [normalize_for_display(t, is_token=is_token, fix_arabic=False) for t in text]\n",
    "            # Add a separator between tokens\n",
    "            return fix_arabic_display(\"â”ƒ\".join(token_list))\n",
    "        else:\n",
    "            # Map function on each element\n",
    "            return [normalize_for_display(t, is_token=is_token, fix_arabic=fix_arabic) for t in text]\n",
    "    \n",
    "    # Escape line breaks and tabs\n",
    "    text = text \\\n",
    "        .replace(\"\\n\", \"\\\\n\") \\\n",
    "        .replace(\"\\t\", \"\\\\t\") \\\n",
    "        .replace(\"\\r\", \"\\\\r\")\n",
    "    \n",
    "    if is_token:\n",
    "        # Standard representation of whitespace\n",
    "        text = text \\\n",
    "            .replace(\" \", \"â–\") \\\n",
    "            .replace(\"Â \", \"\\\\â–\") \\\n",
    "            .replace(\"Ä \", \"â–\")\n",
    "    \n",
    "    # For non-ASCII characters that are NOT encoded in UTF-8\n",
    "    # \"ÃƒÂ©\" -> \"Ã©\", ...\n",
    "    # It happens with the token \"representation strings\" for byte-level models like Bloom, Qwen, Falcon, Llama 2, Olmo 2, C4, Aya \n",
    "    # if tokens are not decoded (should be deprecated with correct implementation)\n",
    "    if \"Ãƒ\" in text or \"Ã¢\" in text:\n",
    "        text = ftfy.fix_text(text, normalization=\"NFC\")\n",
    "    \n",
    "    # Special tokens\n",
    "    for special_in, special_out in {\n",
    "        # Mistral, Llama 2\n",
    "        \"<s>\": \"<BOS>\", \"</s>\": \"<EOS>\",\n",
    "        # C4, Aya\n",
    "        \"<BOS_TOKEN>\": \"<BOS>\", \"<|END_OF_TURN_TOKEN|>\": \"<EOS>\",\n",
    "        # Llama 3\n",
    "        \"<|begin_of_text|>\": \"<BOS>\", \"<|end_of_text|>\": \"<EOS>\",\n",
    "        # Gemma\n",
    "        \"<bos>\": \"<BOS>\", \"<eos>\": \"<EOS>\",\n",
    "        # Whisper\n",
    "        \"<|startoftranscript|>\": \"<BOS>\", \"<|endoftext|>\": \"<EOS>\",\n",
    "        # Nemo\n",
    "        \"â‡\": \"<UNK>\",\n",
    "        # misc. tags\n",
    "        \"<|\": \"<\", \"|>\": \">\",\n",
    "    }.items():\n",
    "        text = text.replace(special_in, special_out)\n",
    "    \n",
    "    if fix_arabic:\n",
    "        text = fix_arabic_display(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# from bidi.algorithm import get_display # Did not find a good thing in python-bidi (?)\n",
    "\n",
    "# Unicode characters for Right-to-Left Mark (RLM) and Left-to-Right Mark (LRM)\n",
    "_RLM = '\\u200F'\n",
    "_LRM = '\\u200E'\n",
    "\n",
    "def is_separator(char):\n",
    "    return char in \"â”ƒ\"\n",
    "\n",
    "def is_arabic(char):\n",
    "    return ord(char) in range(0x600, 0x6ff) or is_separator(char)\n",
    "\n",
    "def is_word(char):\n",
    "    return char.isalpha() or char in \"ØŸØŒ\"\n",
    "\n",
    "def contains_arabic(text):\n",
    "    return any(is_arabic(c) for c in text)\n",
    "\n",
    "def contains_latin(text):\n",
    "    return any(not is_arabic(c) and is_word(c) for c in text)\n",
    "\n",
    "def fix_arabic_display(input, verbose=False):\n",
    "    \"\"\"\n",
    "    Format a string with Arabic and latin characters (code-switching) for a good display in the notebook.\n",
    "    \"\"\"\n",
    "    if isinstance(input, list):\n",
    "        return [fix_arabic_display(i) for i in input]\n",
    "    \n",
    "    # Ignore already converted text\n",
    "    if _RLM in input or _LRM in input:\n",
    "        return input\n",
    "    # Convert only code-switching text\n",
    "    if not contains_arabic(input) or not contains_latin(input):\n",
    "        if contains_arabic(input):\n",
    "            input = _RLM + input\n",
    "        return input\n",
    "    \n",
    "    if not input: return \"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Fixing display for: {input}\")\n",
    "\n",
    "    is_current_arabic = is_arabic(input[0])\n",
    "    chunks_by_language = [\n",
    "        _RLM if is_current_arabic else _LRM\n",
    "    ]\n",
    "    is_space = False\n",
    "    must_add_extra_space = False\n",
    "\n",
    "\n",
    "    for c in input:\n",
    "        was_previous_arabic = is_current_arabic\n",
    "        is_current_arabic = is_arabic(c)\n",
    "        # Special treatment for space (have to be in Arabic chunks)\n",
    "        was_previous_space = is_space\n",
    "        is_space = (c == \" \")\n",
    "        if is_space:\n",
    "            is_current_arabic = True\n",
    "        if is_current_arabic != was_previous_arabic:\n",
    "            if must_add_extra_space:\n",
    "                chunks_by_language[-1] += \" \"\n",
    "            # Add direction switch mark before the character\n",
    "            chunks_by_language.append(_RLM if is_current_arabic else _LRM)\n",
    "            # Add an extra space before the code-switching\n",
    "            must_add_extra_space = was_previous_space\n",
    "\n",
    "        chunks_by_language[-1] += c\n",
    "\n",
    "    if must_add_extra_space:\n",
    "        chunks_by_language[-1] += \" \"\n",
    "\n",
    "    # Reverse the chunk order\n",
    "    chunks_by_language = chunks_by_language[::-1]\n",
    "\n",
    "    # Put separators on the other side (they are affected to Arabic segments)\n",
    "    for i, chunk in enumerate(chunks_by_language):\n",
    "        if is_separator(chunk[-1]) and not is_separator(chunk[0]):\n",
    "            chunks_by_language[i] = chunk[-1] + chunk[:-1]\n",
    "\n",
    "    return \"\".join(chunks_by_language)\n",
    "\n",
    "\n",
    "if \"TEST\":\n",
    "    title = \"â˜ª Test: Fix of display for text with Arabic and code-switching â˜ª\"\n",
    "    print(f\"{title}\\n\" + \"-\"*(len(title)+2))\n",
    "\n",
    "    input = \"Ù…Ø±Ø­Ø¨Ø§Ù‹ Jean-PierreØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\"\n",
    "    inputs = [\n",
    "        input,\n",
    "        \"Jean-PierreØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\",\n",
    "        \"Ù…Ø±Ø­Ø¨Ø§Ù‹ Jean-Pierre\",\n",
    "        # input.replace(\"ØŒ\", \",\").replace(\"ØŸ\", \"?\"),\n",
    "        # \"Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø£Ø­Ù…Ø¯ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\",\n",
    "        # \"Bonjour Jean-Pierre, comment Ã§a va?\",\n",
    "    ]\n",
    "    inputs_for_display = [fix_arabic_display(i) for i in inputs]\n",
    "\n",
    "    for i, (input, display_input) in enumerate(zip(inputs, inputs_for_display)):\n",
    "        if i > 0:\n",
    "            print(\"\\n\", display_input)\n",
    "            continue\n",
    "\n",
    "        print(\"â¡ï¸ Original string\")\n",
    "        print(input)\n",
    "        print(\"â¡ï¸ String for display\")\n",
    "        print(display_input)\n",
    "        print(\"â¡ï¸ Word decomposition (of original string)\")\n",
    "        for i, word in enumerate(input.split()):\n",
    "            print(f\" {i+1:2d}- {fix_arabic_display(word)}\")\n",
    "\n",
    "        # # Uncomment for debugging (see all characters encoded, one by line)\n",
    "        # print(\"â¡ï¸ Character decomposition (of string for display):\")\n",
    "        # for i, c in enumerate(display_input):\n",
    "        #     c = c.replace(_RLM, \"<RLM>\").replace(_LRM, \"<LRM>\")\n",
    "        #     print(f\" {i+1:2d}- {c}\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Original\": inputs,\n",
    "    \"Display\": inputs_for_display,\n",
    "    \"Display as tokens\": [normalize_for_display(input.split(), is_token=True) for input in inputs],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the tokenizer download / loading (which can be long ğŸ¥±)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "for tokenizer in my_tokenizers:\n",
    "    load_tokenizer_with_cache(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test encoding and decoding with a simple string with a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¡ï¸ [1, 8396, 29926, 473, 818, 9411, 1738, 2]\n",
      "â¡ï¸ â€<EOS>â”ƒâ€â€!â”ƒâ€â€tousâ”ƒâ€â€Ã â”ƒâ€â€ourâ”ƒâ€â€jâ”ƒâ€â€Bonâ”ƒâ€â€<BOS>\n",
      "â¡ï¸ <s> Bonjour Ã  tous !</s>\n",
      "â¡ï¸ <BOS> Bonjour Ã  tous !<EOS>\n"
     ]
    }
   ],
   "source": [
    "for output in encode_decode(\n",
    "    \"Llama 2\", # tokenizer\n",
    "    \"Bonjour Ã  tous !\" # input\n",
    "    ):\n",
    "    print(\"â¡ï¸\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘€ See what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pandas` library (just to diplay tables later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.colheader_justify', 'left') # WTF ?\n",
    "# pd.set_option('display.show_index', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğ–¡ See how tokenizers do tokenize strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>size</th>\n",
       "      <th>#tokens</th>\n",
       "      <th>fert.</th>\n",
       "      <th>encoded tokens</th>\n",
       "      <th>round-trip</th>\n",
       "      <th>decoded text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT 3.5</td>\n",
       "      <td>100.3k</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>â€â–!â”ƒâ€â€asâ”ƒâ€â€-bâ”ƒâ€â€â–lÃ â”ƒâ€â€'enâ”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>Mais, maisâ€¦ vas t'en lÃ -bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT 4</td>\n",
       "      <td>100.3k</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>â€â–!â”ƒâ€â€asâ”ƒâ€â€-bâ”ƒâ€â€â–lÃ â”ƒâ€â€'enâ”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>Mais, maisâ€¦ vas t'en lÃ -bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemma</td>\n",
       "      <td>256k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€â€â–!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€&lt;BOS&gt;</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>&lt;BOS&gt;Mais, maisâ€¦ vas t'en lÃ -bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>151.6k</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>â€â–!â”ƒâ€â€asâ”ƒâ€â€-bâ”ƒâ€â€â–lÃ â”ƒâ€â€'enâ”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>Mais, maisâ€¦ vas t'en lÃ -bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Falcon</td>\n",
       "      <td>65k</td>\n",
       "      <td>13</td>\n",
       "      <td>2.2</td>\n",
       "      <td>â€!â”ƒâ€â€â–â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>Mais, maisâ€¦ vas t'en lÃ -bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mistral</td>\n",
       "      <td>32k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€tâ”ƒâ€â€vasâ”ƒâ€â€â€¦â”ƒâ€â€maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€&lt;BOS&gt;</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>&lt;BOS&gt; Mais, maisâ€¦ vas t'en lÃ -bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama 2</td>\n",
       "      <td>32k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€tâ”ƒâ€â€vasâ”ƒâ€â€â€¦â”ƒâ€â€maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€&lt;BOS&gt;</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>&lt;BOS&gt; Mais, maisâ€¦ vas t'en lÃ -bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>128k</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>â€!â”ƒâ€â€asâ”ƒâ€â€-bâ”ƒâ€â€â–lÃ â”ƒâ€â€'enâ”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€&lt;BOS&gt;</td>\n",
       "      <td>âŒ</td>\n",
       "      <td>&lt;BOS&gt;Mais, maisâ€¦ vas t'en lÃ -bas!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Croissant</td>\n",
       "      <td>32k</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€â€!â”ƒâ€â€-basâ”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€t'â”ƒâ€â€vasâ”ƒâ€â€...â”ƒâ€â€maisâ”ƒâ€â€Mais,â”ƒâ€â€&lt;BOS&gt;</td>\n",
       "      <td>âŒ</td>\n",
       "      <td>&lt;BOS&gt; Mais, mais... vas t'en lÃ -bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lucie</td>\n",
       "      <td>65k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€tâ”ƒâ€â€vasâ”ƒâ€â€â€¦â”ƒâ€â€maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€&lt;BOS&gt;</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>&lt;BOS&gt; Mais, maisâ€¦ vas t'en lÃ -bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bloom</td>\n",
       "      <td>250.7k</td>\n",
       "      <td>8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>â€â–!â”ƒâ€â€â–lÃ -basâ”ƒâ€â€â–t'enâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>Mais, maisâ€¦ vas t'en lÃ -bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Olmo 2</td>\n",
       "      <td>100.3k</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>â€â–!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>Mais, maisâ€¦ vas t'en lÃ -bas !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C4</td>\n",
       "      <td>255k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€â€â–!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€&lt;BOS&gt;</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>&lt;BOS&gt;Mais, maisâ€¦ vas t'en lÃ -bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Aya</td>\n",
       "      <td>255k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€â€â–!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€&lt;BOS&gt;</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>&lt;BOS&gt;Mais, maisâ€¦ vas t'en lÃ -bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Jais</td>\n",
       "      <td>64k</td>\n",
       "      <td>14</td>\n",
       "      <td>2.3</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€tâ”ƒâ€â€vasâ”ƒâ€â€â€¦â”ƒâ€â€maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€&lt;BOS&gt;</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>&lt;BOS&gt; Mais, maisâ€¦ vas t'en lÃ -bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EuroLLM</td>\n",
       "      <td>128k</td>\n",
       "      <td>16</td>\n",
       "      <td>2.7</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€tâ”ƒâ€â€vasâ”ƒâ€â€ï¿½â”ƒâ€â€ï¿½â”ƒâ€â€ï¿½â”ƒâ€â€maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€&lt;BOS&gt;</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>&lt;BOS&gt; Mais, maisâ€¦ vas t'en lÃ -bas !&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Whisper</td>\n",
       "      <td>50.3k</td>\n",
       "      <td>16</td>\n",
       "      <td>2.7</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€aisâ”ƒâ€â€Mâ”ƒâ€â€&lt;notimestamps&gt;â”ƒâ€â€&lt;BOS&gt;</td>\n",
       "      <td>âŒ</td>\n",
       "      <td>&lt;BOS&gt;&lt;notimestamps&gt;Mais, maisâ€¦ vas t'en lÃ -bas!&lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Parakeet</td>\n",
       "      <td>1024</td>\n",
       "      <td>17</td>\n",
       "      <td>2.8</td>\n",
       "      <td>â€&lt;unk&gt;â”ƒâ€â€â–â”ƒâ€â€asâ”ƒâ€â€bâ”ƒâ€â€&lt;unk&gt;â”ƒâ€â€â–lâ”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€asâ”ƒâ€â€â–vâ”ƒâ€â€&lt;unk&gt;â”ƒâ€â€isâ”ƒâ€â€â–maâ”ƒâ€â€&lt;unk&gt;â”ƒâ€â€isâ”ƒâ€â€â–ma</td>\n",
       "      <td>âŒ</td>\n",
       "      <td>mais &lt;UNK&gt;  mais &lt;UNK&gt;  vas t'en l &lt;UNK&gt; bas  &lt;UNK&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Perruche</td>\n",
       "      <td>1024</td>\n",
       "      <td>15</td>\n",
       "      <td>2.5</td>\n",
       "      <td>â€&lt;unk&gt;â”ƒâ€â€â–â”ƒâ€â€asâ”ƒâ€â€bâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€asâ”ƒâ€â€â–vâ”ƒâ€â€&lt;unk&gt;â”ƒâ€â€â–maisâ”ƒâ€â€&lt;unk&gt;â”ƒâ€â€â–mais</td>\n",
       "      <td>âŒ</td>\n",
       "      <td>mais &lt;UNK&gt;  mais &lt;UNK&gt;  vas t'en lÃ -bas  &lt;UNK&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokenizer  size     #tokens  fert.  \\\n",
       "0     GPT 3.5  100.3k  11       1.8     \n",
       "1       GPT 4  100.3k  11       1.8     \n",
       "2       Gemma    256k  14       2.3     \n",
       "3        Qwen  151.6k  11       1.8     \n",
       "4      Falcon     65k  13       2.2     \n",
       "5     Mistral     32k  14       2.3     \n",
       "6     Llama 2     32k  14       2.3     \n",
       "7     Llama 3    128k  12       2.0     \n",
       "8   Croissant     32k  11       1.8     \n",
       "9       Lucie     65k  14       2.3     \n",
       "10      Bloom  250.7k   8       1.3     \n",
       "11     Olmo 2  100.3k  12       2.0     \n",
       "12         C4    255k  14       2.3     \n",
       "13        Aya    255k  14       2.3     \n",
       "14       Jais     64k  14       2.3     \n",
       "15    EuroLLM    128k  16       2.7     \n",
       "16    Whisper   50.3k  16       2.7     \n",
       "17   Parakeet    1024  17       2.8     \n",
       "18   Perruche    1024  15       2.5     \n",
       "\n",
       "   encoded tokens                                                                                       \\\n",
       "0                                         â€â–!â”ƒâ€â€asâ”ƒâ€â€-bâ”ƒâ€â€â–lÃ â”ƒâ€â€'enâ”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais   \n",
       "1                                         â€â–!â”ƒâ€â€asâ”ƒâ€â€-bâ”ƒâ€â€â–lÃ â”ƒâ€â€'enâ”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais   \n",
       "2                      â€<EOS>â”ƒâ€â€â–!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€<BOS>   \n",
       "3                                         â€â–!â”ƒâ€â€asâ”ƒâ€â€-bâ”ƒâ€â€â–lÃ â”ƒâ€â€'enâ”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais   \n",
       "4                                   â€!â”ƒâ€â€â–â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais   \n",
       "5                           â€<EOS>â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€tâ”ƒâ€â€vasâ”ƒâ€â€â€¦â”ƒâ€â€maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€<BOS>   \n",
       "6                           â€<EOS>â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€tâ”ƒâ€â€vasâ”ƒâ€â€â€¦â”ƒâ€â€maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€<BOS>   \n",
       "7                                  â€!â”ƒâ€â€asâ”ƒâ€â€-bâ”ƒâ€â€â–lÃ â”ƒâ€â€'enâ”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€<BOS>   \n",
       "8                                  â€<EOS>â”ƒâ€â€!â”ƒâ€â€-basâ”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€t'â”ƒâ€â€vasâ”ƒâ€â€...â”ƒâ€â€maisâ”ƒâ€â€Mais,â”ƒâ€â€<BOS>   \n",
       "9                           â€<EOS>â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€tâ”ƒâ€â€vasâ”ƒâ€â€â€¦â”ƒâ€â€maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€<BOS>   \n",
       "10                                                 â€â–!â”ƒâ€â€â–lÃ -basâ”ƒâ€â€â–t'enâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais   \n",
       "11                                     â€â–!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Mais   \n",
       "12                     â€<EOS>â”ƒâ€â€â–!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€<BOS>   \n",
       "13                     â€<EOS>â”ƒâ€â€â–!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€<BOS>   \n",
       "14                          â€<EOS>â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€tâ”ƒâ€â€vasâ”ƒâ€â€â€¦â”ƒâ€â€maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€<BOS>   \n",
       "15                  â€<EOS>â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€tâ”ƒâ€â€vasâ”ƒâ€â€ï¿½â”ƒâ€â€ï¿½â”ƒâ€â€ï¿½â”ƒâ€â€maisâ”ƒâ€â€,â”ƒâ€â€Maisâ”ƒâ€â€<BOS>   \n",
       "16  â€<EOS>â”ƒâ€â€!â”ƒâ€â€basâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€â–vasâ”ƒâ€â€â€¦â”ƒâ€â€â–maisâ”ƒâ€â€,â”ƒâ€â€aisâ”ƒâ€â€Mâ”ƒâ€â€<notimestamps>â”ƒâ€â€<BOS>   \n",
       "17      â€<unk>â”ƒâ€â€â–â”ƒâ€â€asâ”ƒâ€â€bâ”ƒâ€â€<unk>â”ƒâ€â€â–lâ”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€asâ”ƒâ€â€â–vâ”ƒâ€â€<unk>â”ƒâ€â€isâ”ƒâ€â€â–maâ”ƒâ€â€<unk>â”ƒâ€â€isâ”ƒâ€â€â–ma   \n",
       "18               â€<unk>â”ƒâ€â€â–â”ƒâ€â€asâ”ƒâ€â€bâ”ƒâ€â€-â”ƒâ€â€â–lÃ â”ƒâ€â€enâ”ƒâ€â€'â”ƒâ€â€â–tâ”ƒâ€â€asâ”ƒâ€â€â–vâ”ƒâ€â€<unk>â”ƒâ€â€â–maisâ”ƒâ€â€<unk>â”ƒâ€â€â–mais   \n",
       "\n",
       "   round-trip decoded text                                           \n",
       "0   âœ…                                 Mais, maisâ€¦ vas t'en lÃ -bas !  \n",
       "1   âœ…                                 Mais, maisâ€¦ vas t'en lÃ -bas !  \n",
       "2   âœ…                       <BOS>Mais, maisâ€¦ vas t'en lÃ -bas !<EOS>  \n",
       "3   âœ…                                 Mais, maisâ€¦ vas t'en lÃ -bas !  \n",
       "4   âœ…                                 Mais, maisâ€¦ vas t'en lÃ -bas !  \n",
       "5   âœ…                      <BOS> Mais, maisâ€¦ vas t'en lÃ -bas !<EOS>  \n",
       "6   âœ…                      <BOS> Mais, maisâ€¦ vas t'en lÃ -bas !<EOS>  \n",
       "7   âŒ                             <BOS>Mais, maisâ€¦ vas t'en lÃ -bas!  \n",
       "8   âŒ                    <BOS> Mais, mais... vas t'en lÃ -bas !<EOS>  \n",
       "9   âœ…                      <BOS> Mais, maisâ€¦ vas t'en lÃ -bas !<EOS>  \n",
       "10  âœ…                                 Mais, maisâ€¦ vas t'en lÃ -bas !  \n",
       "11  âœ…                                 Mais, maisâ€¦ vas t'en lÃ -bas !  \n",
       "12  âœ…                       <BOS>Mais, maisâ€¦ vas t'en lÃ -bas !<EOS>  \n",
       "13  âœ…                       <BOS>Mais, maisâ€¦ vas t'en lÃ -bas !<EOS>  \n",
       "14  âœ…                      <BOS> Mais, maisâ€¦ vas t'en lÃ -bas !<EOS>  \n",
       "15  âœ…                      <BOS> Mais, maisâ€¦ vas t'en lÃ -bas !<EOS>  \n",
       "16  âŒ          <BOS><notimestamps>Mais, maisâ€¦ vas t'en lÃ -bas!<EOS>  \n",
       "17  âŒ          mais <UNK>  mais <UNK>  vas t'en l <UNK> bas  <UNK>   \n",
       "18  âŒ               mais <UNK>  mais <UNK>  vas t'en lÃ -bas  <UNK>   "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def test_tokenizers_on_text(\n",
    "        text,\n",
    "        tokenizers=my_tokenizers,\n",
    "        display_vocabulary_size=True,\n",
    "        display_fertility=True,\n",
    "        display_round_trip_result=True,\n",
    "        ):\n",
    "    # Build a table with the encoded and decoded text for each tokenizer\n",
    "    all_data = []\n",
    "    for tokenizer in tokenizers:\n",
    "        tokens, tokens_str, decoded_raw, decoded = encode_decode(tokenizer, text)\n",
    "        N = vocabulary_size(tokenizer)\n",
    "        data = {\n",
    "            \"tokenizer\": tokenizer,\n",
    "        }\n",
    "        if display_vocabulary_size:\n",
    "            data[\"size\"] = format_thoushands(N)\n",
    "        if display_fertility:\n",
    "            num_words = len(text.split())\n",
    "            num_tokens = len(tokens)\n",
    "            if tokens_str.startswith(\"<BOS>\"):\n",
    "                num_tokens -= 1\n",
    "            if tokens_str.endswith(\"<EOS>\"):\n",
    "                num_tokens -= 1\n",
    "            data[\"#tokens\"] = num_tokens\n",
    "            data[\"fert.\"] = round(num_tokens / num_words, 1)\n",
    "        data[\"encoded tokens\"]= tokens_str\n",
    "        if display_round_trip_result:\n",
    "            # Remove all tags in brackets (like <BOS>, <EOS>, <UNK>, ...)\n",
    "            normalized_decoded = re.sub(r\"<[^>]*>\", \"\", decoded_raw)\n",
    "            data[\"round-trip\"] = \"âœ…\" if normalized_decoded.strip() == text.strip() else \"âŒ\"\n",
    "        data[\"decoded text\"]= decoded\n",
    "        all_data.append(data)\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def format_thoushands(N):\n",
    "    import math\n",
    "    if N < 1100:\n",
    "        return str(N)\n",
    "    if N - 1000 * math.floor(N/1000) < 100:\n",
    "        return f\"{N//1000}k\"\n",
    "    return f\"{N/1000:.1f}k\"\n",
    "    \n",
    "\n",
    "test_tokenizers_on_text(\"Mais, maisâ€¦ vas t'en lÃ -bas !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğ–¡ See how tokenizers do tokenize strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“š See vocabulary of (small) tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>nchars</th>\n",
       "      <th>start</th>\n",
       "      <th>#tokens</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>â–</td>\n",
       "      <td>1</td>\n",
       "      <td>[â–]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#</td>\n",
       "      <td>27</td>\n",
       "      <td>[', a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>â–</td>\n",
       "      <td>25</td>\n",
       "      <td>[â–', â–a, â–b, â–c, â–d, â–e, â–f, â–g, â–h, â–i, â–j, â–k, â–l, â–m, â–n, â–o, â–p, â–r, â–s, â–t, â–u, â–v, â–w, â–y, â–z]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>#</td>\n",
       "      <td>119</td>\n",
       "      <td>[ab, ac, ad, ag, ah, ak, al, am, an, ap, ar, as, at, av, ay, be, ce, ch, ci, ck, co, ct, cy, de, du, ed, ef, el, em, en, ep, er, es, et, ew, fe, ff, ft, ge, gg, gn, ht, ia, ib, ic, id, ie, if, ig, il, im, in, ip, ir, is, it, iv, ix, iz, ke, ks, ld, le, li, ll, ly, me, mo, na, nd, ne, nt, od, og, ol, om, on, oo, op, or, os, ot, ou, ow, oy, pe, ph, pl, pp, ps, pt, qu, ra, re, ri, ro, ru, ry, se, so, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>â–</td>\n",
       "      <td>114</td>\n",
       "      <td>[â–ab, â–ac, â–ad, â–af, â–ag, â–ah, â–al, â–am, â–an, â–ap, â–ar, â–as, â–at, â–be, â–bl, â–bo, â–br, â–bu, â–by, â–ca, â–ch, â–cl, â–co, â–cr, â–de, â–do, â–dr, â–ed, â–el, â–em, â–en, â–es, â–eu, â–ev, â–ex, â–fa, â–fe, â–fl, â–fo, â–fr, â–gl, â–go, â–gr, â–gu, â–ha, â–he, â–ho, â–hu, â–if, â–im, â–in, â–is, â–it, â–jo, â–ke, â–kn, â–la, â–le, â–li, â–lo, â–ma, â–me, â–mo, â–mr, â–mu, â–my, â–ne, â–no, â–ob, â–of, â–oh, â–ok, â–on, â–op, â–or, â–pa, â–pe, â–ph, â–pl, â–po, â–pr, â–qu, â–ra, â–re, â–ro, â–sa, â–sc, â–se, â–sh, â–sk, â–sl, â–sm, â–so, â–sp, â–st, â–su, â–sy, â–ta, â–te, â–th, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>#</td>\n",
       "      <td>151</td>\n",
       "      <td>[ace, ach, ack, act, ade, ady, age, ail, aim, ain, all, als, ame, and, ang, ank, ans, ant, ard, are, ark, ars, art, ary, ase, ash, ass, ast, ate, ath, ave, ber, ble, ced, ces, con, cri, cus, day, der, ead, ect, ell, ens, ent, ere, erm, ers, ert, ess, est, ets, ety, ful, ger, her, hip, ial, ian, ice, ich, ick, ics, ict, ide, ied, ies, iew, iff, igh, ign, ild, ile, ill, ily, ind, ine, ing, ink, int, ion, ire, ise, ish, iss, ist, ite, ith, its, itt, ity, ive, ize, kes, led, les, lic, lud, nce, nds, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>â–</td>\n",
       "      <td>174</td>\n",
       "      <td>[â–acc, â–act, â–add, â–adv, â–all, â–and, â–any, â–app, â–are, â–arg, â–art, â–ask, â–ass, â–att, â–aut, â–bas, â–bec, â–beg, â–beh, â–bel, â–bet, â–big, â–bit, â–bra, â–bre, â–bus, â–but, â–can, â–car, â–che, â–cle, â–col, â–com, â–con, â–cor, â–cou, â–cre, â–cur, â–day, â–dec, â–def, â–des, â–det, â–did, â–dis, â–don, â–ear, â–eas, â–eff, â–ele, â–end, â–eng, â–ent, â–exp, â–ext, â–fam, â–far, â–few, â–fin, â–fir, â–for, â–fun, â–gen, â–get, â–god, â–got, â–gra, â–had, â–has, â–hel, â–her, â–him, â–his, â–hon, â–how, â–hum, â–ide, â–imp, â–inc, â–ind, â–inf, â–ins, â–int, â–inv, â–iss, â–its, â–jud, â–lar, â–law, â–leg, â–let, â–loc, â–lot, â–man, â–mar, â–may, â–med, â–mem, â–met, â–min, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>#</td>\n",
       "      <td>75</td>\n",
       "      <td>[able, ably, aint, ally, alth, ance, ange, arch, ason, atch, ated, ater, ates, ause, blem, body, cept, cess, cial, ence, ense, ents, enty, ered, eric, ever, fore, form, hing, ible, ical, ices, ient, ific, ight, ines, ings, ions, ious, ited, ject, king, llow, ment, nder, ness, ning, olog, oney, onna, osed, ough, ould, ound, ount, ouse, ower, pect, ract, reat, ress, ross, self, stem, ther, ting, ular, ures, vern, vers, very, ving, ward, ways, ween]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>â–</td>\n",
       "      <td>158</td>\n",
       "      <td>[â–able, â–also, â–appe, â–away, â–back, â–been, â–best, â–book, â–both, â–call, â–came, â–care, â–case, â–cent, â–char, â–coll, â–come, â–comm, â–comp, â–conc, â–conf, â–cons, â–cont, â–cour, â–didn, â–diff, â–dire, â–dist, â–does, â–done, â–down, â–each, â–even, â–ever, â–exam, â–expl, â–fact, â–feel, â–find, â–five, â–form, â–four, â–frie, â–from, â–full, â–give, â–good, â–hand, â–happ, â–hard, â–have, â–head, â–hear, â–help, â–here, â–high, â–home, â–hund, â–inst, â–into, â–just, â–keep, â–kind, â–know, â–last, â–lead, â–lear, â–left, â–life, â–like, â–list, â–long, â–look, â–love, â–made, â–make, â–many, â–mark, â–mean, â–miss, â–more, â–most, â–move, â–much, â–must, â–name, â–need, â–next, â–okay, â–once, â–only, â–open, â–over, â–part, â–pass, â–peop, â–pers, â–plan, â–play, â–poss, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>#</td>\n",
       "      <td>26</td>\n",
       "      <td>[ately, ather, ating, ation, ative, atter, ature, ction, erest, ether, ident, ility, iness, ision, ities, ition, ittle, ments, other, ought, ready, thing, ually, uring, ution, velop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>â–</td>\n",
       "      <td>70</td>\n",
       "      <td>[â–about, â–after, â–again, â–allow, â–being, â–belie, â–build, â–child, â–claim, â–clear, â–could, â–count, â–court, â–creat, â–doesn, â–doing, â–eight, â–every, â–exper, â–first, â–found, â–gener, â–going, â–gonna, â–great, â–house, â–inter, â–light, â–maybe, â–means, â–might, â–money, â–never, â–order, â–other, â–place, â–point, â–power, â–produ, â–quest, â–right, â–small, â–somet, â–stand, â–start, â–state, â–still, â–thank, â–their, â–there, â–these, â–thing, â–think, â–those, â–thous, â–three, â–today, â–trans, â–under, â–using, â–video, â–watch, â–water, â–where, â–which, â–while, â–whole, â–world, â–would, â–years]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>#</td>\n",
       "      <td>5</td>\n",
       "      <td>[ations, ention, ertain, ically, idence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>â–</td>\n",
       "      <td>42</td>\n",
       "      <td>[â–&lt;UNK&gt;â–, â–always, â–americ, â–around, â–before, â–better, â–called, â–change, â–commun, â–compan, â–comple, â–consid, â–contin, â–course, â–enough, â–experi, â–family, â–follow, â–govern, â–happen, â–having, â–import, â–includ, â–inform, â–little, â–making, â–number, â–partic, â–people, â–person, â–pretty, â–really, â–reason, â–record, â–saying, â–school, â–second, â–should, â–system, â–things, â–though, â–trying]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>#</td>\n",
       "      <td>1</td>\n",
       "      <td>[ational]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>â–</td>\n",
       "      <td>21</td>\n",
       "      <td>[â–already, â–another, â–because, â–believe, â–between, â–certain, â–develop, â–differe, â–example, â–getting, â–hundred, â–looking, â–problem, â–process, â–support, â–thought, â–through, â–underst, â–whether, â–without, â–working]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>â–</td>\n",
       "      <td>8</td>\n",
       "      <td>[â–actually, â–anything, â–business, â–interest, â–probably, â–question, â–thousand, â–together]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>â–</td>\n",
       "      <td>3</td>\n",
       "      <td>[â–different, â–important, â–something]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>â–</td>\n",
       "      <td>3</td>\n",
       "      <td>[â–everything, â–government, â–understand]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>â–</td>\n",
       "      <td>1</td>\n",
       "      <td>[â–information]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td></td>\n",
       "      <td>1024</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nchars start  #tokens  \\\n",
       "0       0  â–        1      \n",
       "1       1  #       27      \n",
       "2       1  â–       25      \n",
       "3       2  #      119      \n",
       "4       2  â–      114      \n",
       "5       3  #      151      \n",
       "6       3  â–      174      \n",
       "7       4  #       75      \n",
       "8       4  â–      158      \n",
       "9       5  #       26      \n",
       "10      5  â–       70      \n",
       "11      6  #        5      \n",
       "12      6  â–       42      \n",
       "13      7  #        1      \n",
       "14      7  â–       21      \n",
       "15      8  â–        8      \n",
       "16      9  â–        3      \n",
       "17     10  â–        3      \n",
       "18     11  â–        1      \n",
       "19  TOTAL        1024      \n",
       "\n",
       "   tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [â–]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [', a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [â–', â–a, â–b, â–c, â–d, â–e, â–f, â–g, â–h, â–i, â–j, â–k, â–l, â–m, â–n, â–o, â–p, â–r, â–s, â–t, â–u, â–v, â–w, â–y, â–z]  \n",
       "3                                                                                                                                                                                                                                                                                                               [ab, ac, ad, ag, ah, ak, al, am, an, ap, ar, as, at, av, ay, be, ce, ch, ci, ck, co, ct, cy, de, du, ed, ef, el, em, en, ep, er, es, et, ew, fe, ff, ft, ge, gg, gn, ht, ia, ib, ic, id, ie, if, ig, il, im, in, ip, ir, is, it, iv, ix, iz, ke, ks, ld, le, li, ll, ly, me, mo, na, nd, ne, nt, od, og, ol, om, on, oo, op, or, os, ot, ou, ow, oy, pe, ph, pl, pp, ps, pt, qu, ra, re, ri, ro, ru, ry, se, so, ...]  \n",
       "4                                                                                                                                                                                                           [â–ab, â–ac, â–ad, â–af, â–ag, â–ah, â–al, â–am, â–an, â–ap, â–ar, â–as, â–at, â–be, â–bl, â–bo, â–br, â–bu, â–by, â–ca, â–ch, â–cl, â–co, â–cr, â–de, â–do, â–dr, â–ed, â–el, â–em, â–en, â–es, â–eu, â–ev, â–ex, â–fa, â–fe, â–fl, â–fo, â–fr, â–gl, â–go, â–gr, â–gu, â–ha, â–he, â–ho, â–hu, â–if, â–im, â–in, â–is, â–it, â–jo, â–ke, â–kn, â–la, â–le, â–li, â–lo, â–ma, â–me, â–mo, â–mr, â–mu, â–my, â–ne, â–no, â–ob, â–of, â–oh, â–ok, â–on, â–op, â–or, â–pa, â–pe, â–ph, â–pl, â–po, â–pr, â–qu, â–ra, â–re, â–ro, â–sa, â–sc, â–se, â–sh, â–sk, â–sl, â–sm, â–so, â–sp, â–st, â–su, â–sy, â–ta, â–te, â–th, ...]  \n",
       "5                                                                                                                                                                                                           [ace, ach, ack, act, ade, ady, age, ail, aim, ain, all, als, ame, and, ang, ank, ans, ant, ard, are, ark, ars, art, ary, ase, ash, ass, ast, ate, ath, ave, ber, ble, ced, ces, con, cri, cus, day, der, ead, ect, ell, ens, ent, ere, erm, ers, ert, ess, est, ets, ety, ful, ger, her, hip, ial, ian, ice, ich, ick, ics, ict, ide, ied, ies, iew, iff, igh, ign, ild, ile, ill, ily, ind, ine, ing, ink, int, ion, ire, ise, ish, iss, ist, ite, ith, its, itt, ity, ive, ize, kes, led, les, lic, lud, nce, nds, ...]  \n",
       "6                                                                                                       [â–acc, â–act, â–add, â–adv, â–all, â–and, â–any, â–app, â–are, â–arg, â–art, â–ask, â–ass, â–att, â–aut, â–bas, â–bec, â–beg, â–beh, â–bel, â–bet, â–big, â–bit, â–bra, â–bre, â–bus, â–but, â–can, â–car, â–che, â–cle, â–col, â–com, â–con, â–cor, â–cou, â–cre, â–cur, â–day, â–dec, â–def, â–des, â–det, â–did, â–dis, â–don, â–ear, â–eas, â–eff, â–ele, â–end, â–eng, â–ent, â–exp, â–ext, â–fam, â–far, â–few, â–fin, â–fir, â–for, â–fun, â–gen, â–get, â–god, â–got, â–gra, â–had, â–has, â–hel, â–her, â–him, â–his, â–hon, â–how, â–hum, â–ide, â–imp, â–inc, â–ind, â–inf, â–ins, â–int, â–inv, â–iss, â–its, â–jud, â–lar, â–law, â–leg, â–let, â–loc, â–lot, â–man, â–mar, â–may, â–med, â–mem, â–met, â–min, ...]  \n",
       "7                                                                                                                                                                                                                                                                  [able, ably, aint, ally, alth, ance, ange, arch, ason, atch, ated, ater, ates, ause, blem, body, cept, cess, cial, ence, ense, ents, enty, ered, eric, ever, fore, form, hing, ible, ical, ices, ient, ific, ight, ines, ings, ions, ious, ited, ject, king, llow, ment, nder, ness, ning, olog, oney, onna, osed, ough, ould, ound, ount, ouse, ower, pect, ract, reat, ress, ross, self, stem, ther, ting, ular, ures, vern, vers, very, ving, ward, ways, ween]  \n",
       "8   [â–able, â–also, â–appe, â–away, â–back, â–been, â–best, â–book, â–both, â–call, â–came, â–care, â–case, â–cent, â–char, â–coll, â–come, â–comm, â–comp, â–conc, â–conf, â–cons, â–cont, â–cour, â–didn, â–diff, â–dire, â–dist, â–does, â–done, â–down, â–each, â–even, â–ever, â–exam, â–expl, â–fact, â–feel, â–find, â–five, â–form, â–four, â–frie, â–from, â–full, â–give, â–good, â–hand, â–happ, â–hard, â–have, â–head, â–hear, â–help, â–here, â–high, â–home, â–hund, â–inst, â–into, â–just, â–keep, â–kind, â–know, â–last, â–lead, â–lear, â–left, â–life, â–like, â–list, â–long, â–look, â–love, â–made, â–make, â–many, â–mark, â–mean, â–miss, â–more, â–most, â–move, â–much, â–must, â–name, â–need, â–next, â–okay, â–once, â–only, â–open, â–over, â–part, â–pass, â–peop, â–pers, â–plan, â–play, â–poss, ...]  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [ately, ather, ating, ation, ative, atter, ature, ction, erest, ether, ident, ility, iness, ision, ities, ition, ittle, ments, other, ought, ready, thing, ually, uring, ution, velop]  \n",
       "10                                                                                                                                                   [â–about, â–after, â–again, â–allow, â–being, â–belie, â–build, â–child, â–claim, â–clear, â–could, â–count, â–court, â–creat, â–doesn, â–doing, â–eight, â–every, â–exper, â–first, â–found, â–gener, â–going, â–gonna, â–great, â–house, â–inter, â–light, â–maybe, â–means, â–might, â–money, â–never, â–order, â–other, â–place, â–point, â–power, â–produ, â–quest, â–right, â–small, â–somet, â–stand, â–start, â–state, â–still, â–thank, â–their, â–there, â–these, â–thing, â–think, â–those, â–thous, â–three, â–today, â–trans, â–under, â–using, â–video, â–watch, â–water, â–where, â–which, â–while, â–whole, â–world, â–would, â–years]  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [ations, ention, ertain, ically, idence]  \n",
       "12                                                                                                                                                                                                                                                                                                                                         [â–<UNK>â–, â–always, â–americ, â–around, â–before, â–better, â–called, â–change, â–commun, â–compan, â–comple, â–consid, â–contin, â–course, â–enough, â–experi, â–family, â–follow, â–govern, â–happen, â–having, â–import, â–includ, â–inform, â–little, â–making, â–number, â–partic, â–people, â–person, â–pretty, â–really, â–reason, â–record, â–saying, â–school, â–second, â–should, â–system, â–things, â–though, â–trying]  \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [ational]  \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [â–already, â–another, â–because, â–believe, â–between, â–certain, â–develop, â–differe, â–example, â–getting, â–hundred, â–looking, â–problem, â–process, â–support, â–thought, â–through, â–underst, â–whether, â–without, â–working]  \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [â–actually, â–anything, â–business, â–interest, â–probably, â–question, â–thousand, â–together]  \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [â–different, â–important, â–something]  \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [â–everything, â–government, â–understand]  \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [â–information]  \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vocabulary(\"Parakeet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>nchars</th>\n",
       "      <th>start</th>\n",
       "      <th>#tokens</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>â–</td>\n",
       "      <td>1</td>\n",
       "      <td>[â–]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#</td>\n",
       "      <td>40</td>\n",
       "      <td>[', -, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, Ã , Ã¢, Ã§, Ã¨, Ã©, Ãª, Ã«, Ã®, Ã¯, Ã´, Ã¹, Ã»]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>â–</td>\n",
       "      <td>26</td>\n",
       "      <td>[â–a, â–b, â–c, â–d, â–e, â–f, â–g, â–h, â–i, â–j, â–k, â–l, â–m, â–n, â–o, â–p, â–r, â–s, â–t, â–u, â–v, â–w, â–y, â–z, â–Ã , â–Ã©]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>#</td>\n",
       "      <td>125</td>\n",
       "      <td>[ab, ac, ad, ag, ai, al, am, an, ap, ar, as, at, au, av, ay, aÃ®, be, ce, ch, ci, ct, cu, cÃ©, de, di, du, dÃ©, el, em, en, er, es, eu, ez, ff, fi, fs, ge, gn, gu, ic, ie, if, ig, il, im, in, ir, is, it, je, la, le, li, ll, lo, ls, lÃ , lÃ¨, lÃ©, mb, me, mi, mp, mÃ©, ne, ob, oc, og, oi, ol, om, on, op, or, os, ot, ou, oy, pe, ph, pp, pr, pt, qu, ra, re, ri, rÃ©, se, si, ta, te, th, ti, tr, ts, tt, tu, tÃ©, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>â–</td>\n",
       "      <td>110</td>\n",
       "      <td>[â–ab, â–ac, â–ad, â–ag, â–ah, â–ai, â–al, â–am, â–an, â–ap, â–ar, â–as, â–au, â–av, â–ba, â–be, â–bi, â–bl, â–br, â–ca, â–ce, â–ch, â–cl, â–co, â–cr, â–cÃ´, â–de, â–di, â–dr, â–du, â–dÃ©, â–en, â–es, â–et, â–eu, â–ex, â–fa, â–fi, â–fr, â–gr, â–he, â–hm, â–id, â–il, â–im, â–in, â–je, â–ju, â–la, â–le, â–li, â–lu, â–lÃ , â–ma, â–me, â–mi, â–mo, â–mÃ©, â–ne, â–ni, â–no, â–nÃ©, â–ob, â–oc, â–oh, â–on, â–op, â–or, â–ou, â–oÃ¹, â–pa, â–pe, â–ph, â–pi, â–pl, â–po, â–pr, â–qu, â–ra, â–re, â–ri, â–rÃ©, â–sa, â–sc, â–se, â–si, â–so, â–sp, â–st, â–su, â–sy, â–sÃ©, â–sÃ», â–ta, â–te, â–th, â–ti, â–tr, â–tu, â–tÃ©, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>#</td>\n",
       "      <td>151</td>\n",
       "      <td>[ace, act, age, agn, ain, ais, ait, ale, all, ame, anc, and, ang, ans, ant, ard, ari, ass, ati, ats, aut, aux, ble, bre, cer, ces, che, chÃ©, cin, cip, cle, cti, cul, dem, der, des, dre, end, ens, ent, ers, ert, euf, eur, eux, gne, hui, ici, ien, ier, ies, ign, ill, ine, ing, ins, ion, ise, iss, ist, itu, itÃ©, jet, ler, les, lic, lig, lle, lui, mer, mes, min, mis, mps, ner, nes, oin, oir, ois, oit, ole, olu, omb, omp, ond, one, ong, onn, ons, ont, ord, ore, orm, ors, ort, ose, oup, our, ous, out, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>â–</td>\n",
       "      <td>166</td>\n",
       "      <td>[â–acc, â–aff, â–ain, â–all, â–ann, â–ans, â–app, â–arr, â–ass, â–att, â–auc, â–aus, â–aut, â–aux, â–ave, â–bah, â–ben, â–bes, â–bon, â–bou, â–cap, â–car, â–cas, â–ces, â–cet, â–cha, â–che, â–cin, â–cli, â–com, â–con, â–cor, â–cou, â–crÃ©, â–dem, â–der, â–des, â–dev, â–dis, â–dit, â–dix, â–don, â–dou, â–dÃ©b, â–dÃ©c, â–dÃ©f, â–dÃ©j, â–dÃ©m, â–dÃ©p, â–eff, â–emp, â–enc, â–enf, â–ens, â–ent, â–env, â–esp, â–ess, â–est, â–euh, â–eur, â–eux, â–exe, â–exp, â–fam, â–fem, â–fer, â–fil, â–fin, â–fon, â–for, â–fut, â–gra, â–gÃ©n, â–hab, â–hom, â–hum, â–ici, â–ils, â–imp, â–inc, â–inf, â–ins, â–jam, â–jou, â–jus, â–les, â–lui, â–mad, â–mal, â–man, â–mar, â–mer, â–mes, â–mil, â–min, â–mis, â–moi, â–mom, â–mon, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>#</td>\n",
       "      <td>88</td>\n",
       "      <td>[able, ages, agne, aine, ains, aire, aiss, ales, alis, ance, ande, ange, anis, ante, ants, arde, asse, atre, cher, ches, coup, dent, duit, elle, ence, ends, enir, enne, ense, ente, ents, eure, eurs, euse, iens, ille, ingt, ions, ique, iste, iter, itÃ©s, iÃ¨me, iÃ¨re, jour, lier, lles, lopp, mble, ment, mple, nent, oins, oire, olog, onne, onom, orte, orti, oses, otre, ours, ouve, prÃ¨s, puis, quer, ques, quoi, rais, rait, rent, ress, sion, tant, tion, tout, tres, ttre, uite, ures, veau, vent, vers, voir, vous, Ã¨res, Ãªtes, Ãªtre]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>â–</td>\n",
       "      <td>134</td>\n",
       "      <td>[â–alle, â–appr, â–arri, â–avec, â–avez, â–beau, â–bien, â–cela, â–cent, â–cert, â–ceux, â–char, â–cher, â–chez, â–cinq, â–comb, â–comm, â–comp, â–conc, â–conf, â–conn, â–cons, â–cont, â–coup, â–cour, â–cÃ´tÃ©, â–dans, â–dern, â–deux, â–dieu, â–diff, â–dire, â–doit, â–donc, â–donn, â–dont, â–dÃ©jÃ , â–effe, â–elle, â–ense, â–fais, â–fait, â–faut, â–fois, â–fond, â–form, â–fran, â–gens, â–gros, â–hein, â–hist, â–indi, â–intÃ©, â–jour, â–leur, â–long, â–lors, â–main, â–mais, â–mois, â–mont, â–mort, â–mÃªme, â–neuf, â–nous, â–parf, â–part, â–pass, â–pays, â–pens, â–pers, â–peti, â–peut, â–peux, â–plan, â–plus, â–plut, â–poin, â–poli, â–pour, â–pouv, â–pres, â–prin, â–pris, â–prob, â–prof, â–prop, â–prÃ©s, â–puis, â–quar, â–quel, â–ques, â–quoi, â–rais, â–rapp, â–repr, â–retr, â–rien, â–sain, â–sais, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>#</td>\n",
       "      <td>25</td>\n",
       "      <td>[ables, aient, aines, aires, alitÃ©, ances, anger, ation, ature, ction, endre, eures, ieurs, ilitÃ©, iques, jourd, jours, lique, ments, ouver, ouvez, sible, sieur, tions, tique]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>â–</td>\n",
       "      <td>65</td>\n",
       "      <td>[â–ainsi, â–aller, â–allez, â–alors, â–aprÃ¨s, â–assez, â–aussi, â–autre, â–avais, â–avait, â–avant, â–avoir, â–avons, â–bonne, â–celui, â–cette, â–chose, â–cinqu, â–comme, â–compr, â–contr, â–crois, â–droit, â–elles, â–enfin, â–entre, â–europ, â–faire, â–franÃ§, â–grand, â–heure, â–homme, â–inter, â–jours, â–jusqu, â–juste, â–leurs, â–mieux, â–mille, â–milli, â–moins, â–monde, â–notre, â–nouve, â–ouais, â–parce, â–parle, â–parti, â–passe, â–pense, â–perme, â–petit, â–place, â–point, â–premi, â–quand, â–temps, â–toute, â–trans, â–trois, â–vidÃ©o, â–vingt, â–voilÃ , â–votre, â–Ã©tait]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>#</td>\n",
       "      <td>11</td>\n",
       "      <td>[amment, ations, atique, endant, lement, lleurs, nement, sition, tement, velopp, vement]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>â–</td>\n",
       "      <td>41</td>\n",
       "      <td>[â–&lt;UNK&gt;â–, â–accord, â–annÃ©es, â–autres, â–besoin, â–chaque, â–choses, â–commen, â–commun, â–compte, â–contin, â–contre, â–demand, â–depuis, â–diffÃ©r, â–donner, â–encore, â–europÃ©, â–france, â–grande, â–import, â–jamais, â–mettre, â–moment, â–niveau, â–nombre, â–parler, â–partie, â–passer, â–petite, â–pouvez, â–quatre, â–regard, â–rÃ©pond, â–savoir, â–simple, â–toutes, â–travai, â–trente, â–trouve, â–utilis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>#</td>\n",
       "      <td>2</td>\n",
       "      <td>[alement, llement]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>â–</td>\n",
       "      <td>19</td>\n",
       "      <td>[â–appelle, â–aujourd, â–comment, â–enfants, â–ensuite, â–entrepr, â–exemple, â–mainten, â–pendant, â–personn, â–pouvoir, â–premier, â–prendre, â–prÃ©sent, â–quelque, â–souvent, â–surtout, â–travail, â–Ã©taient]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>â–</td>\n",
       "      <td>14</td>\n",
       "      <td>[â–ailleurs, â–beaucoup, â–fonction, â–franÃ§ais, â–histoire, â–monsieur, â–personne, â–pourquoi, â–premiÃ¨re, â–quelques, â–question, â–soixante, â–toujours, â–vraiment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>â–</td>\n",
       "      <td>5</td>\n",
       "      <td>[â–cinquante, â–important, â–personnes, â–plusieurs, â–Ã©galement]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>â–</td>\n",
       "      <td>1</td>\n",
       "      <td>[â–maintenant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td></td>\n",
       "      <td>1024</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nchars start  #tokens  \\\n",
       "0       0  â–        1      \n",
       "1       1  #       40      \n",
       "2       1  â–       26      \n",
       "3       2  #      125      \n",
       "4       2  â–      110      \n",
       "5       3  #      151      \n",
       "6       3  â–      166      \n",
       "7       4  #       88      \n",
       "8       4  â–      134      \n",
       "9       5  #       25      \n",
       "10      5  â–       65      \n",
       "11      6  #       11      \n",
       "12      6  â–       41      \n",
       "13      7  #        2      \n",
       "14      7  â–       19      \n",
       "15      8  â–       14      \n",
       "16      9  â–        5      \n",
       "17     10  â–        1      \n",
       "18  TOTAL        1024      \n",
       "\n",
       "   tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [â–]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [', -, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, Ã , Ã¢, Ã§, Ã¨, Ã©, Ãª, Ã«, Ã®, Ã¯, Ã´, Ã¹, Ã»]  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [â–a, â–b, â–c, â–d, â–e, â–f, â–g, â–h, â–i, â–j, â–k, â–l, â–m, â–n, â–o, â–p, â–r, â–s, â–t, â–u, â–v, â–w, â–y, â–z, â–Ã , â–Ã©]  \n",
       "3                                                                                                                                                                                                                                                                                                               [ab, ac, ad, ag, ai, al, am, an, ap, ar, as, at, au, av, ay, aÃ®, be, ce, ch, ci, ct, cu, cÃ©, de, di, du, dÃ©, el, em, en, er, es, eu, ez, ff, fi, fs, ge, gn, gu, ic, ie, if, ig, il, im, in, ir, is, it, je, la, le, li, ll, lo, ls, lÃ , lÃ¨, lÃ©, mb, me, mi, mp, mÃ©, ne, ob, oc, og, oi, ol, om, on, op, or, os, ot, ou, oy, pe, ph, pp, pr, pt, qu, ra, re, ri, rÃ©, se, si, ta, te, th, ti, tr, ts, tt, tu, tÃ©, ...]  \n",
       "4                                                                                                                                                                                                           [â–ab, â–ac, â–ad, â–ag, â–ah, â–ai, â–al, â–am, â–an, â–ap, â–ar, â–as, â–au, â–av, â–ba, â–be, â–bi, â–bl, â–br, â–ca, â–ce, â–ch, â–cl, â–co, â–cr, â–cÃ´, â–de, â–di, â–dr, â–du, â–dÃ©, â–en, â–es, â–et, â–eu, â–ex, â–fa, â–fi, â–fr, â–gr, â–he, â–hm, â–id, â–il, â–im, â–in, â–je, â–ju, â–la, â–le, â–li, â–lu, â–lÃ , â–ma, â–me, â–mi, â–mo, â–mÃ©, â–ne, â–ni, â–no, â–nÃ©, â–ob, â–oc, â–oh, â–on, â–op, â–or, â–ou, â–oÃ¹, â–pa, â–pe, â–ph, â–pi, â–pl, â–po, â–pr, â–qu, â–ra, â–re, â–ri, â–rÃ©, â–sa, â–sc, â–se, â–si, â–so, â–sp, â–st, â–su, â–sy, â–sÃ©, â–sÃ», â–ta, â–te, â–th, â–ti, â–tr, â–tu, â–tÃ©, ...]  \n",
       "5                                                                                                                                                                                                           [ace, act, age, agn, ain, ais, ait, ale, all, ame, anc, and, ang, ans, ant, ard, ari, ass, ati, ats, aut, aux, ble, bre, cer, ces, che, chÃ©, cin, cip, cle, cti, cul, dem, der, des, dre, end, ens, ent, ers, ert, euf, eur, eux, gne, hui, ici, ien, ier, ies, ign, ill, ine, ing, ins, ion, ise, iss, ist, itu, itÃ©, jet, ler, les, lic, lig, lle, lui, mer, mes, min, mis, mps, ner, nes, oin, oir, ois, oit, ole, olu, omb, omp, ond, one, ong, onn, ons, ont, ord, ore, orm, ors, ort, ose, oup, our, ous, out, ...]  \n",
       "6                                                                                                       [â–acc, â–aff, â–ain, â–all, â–ann, â–ans, â–app, â–arr, â–ass, â–att, â–auc, â–aus, â–aut, â–aux, â–ave, â–bah, â–ben, â–bes, â–bon, â–bou, â–cap, â–car, â–cas, â–ces, â–cet, â–cha, â–che, â–cin, â–cli, â–com, â–con, â–cor, â–cou, â–crÃ©, â–dem, â–der, â–des, â–dev, â–dis, â–dit, â–dix, â–don, â–dou, â–dÃ©b, â–dÃ©c, â–dÃ©f, â–dÃ©j, â–dÃ©m, â–dÃ©p, â–eff, â–emp, â–enc, â–enf, â–ens, â–ent, â–env, â–esp, â–ess, â–est, â–euh, â–eur, â–eux, â–exe, â–exp, â–fam, â–fem, â–fer, â–fil, â–fin, â–fon, â–for, â–fut, â–gra, â–gÃ©n, â–hab, â–hom, â–hum, â–ici, â–ils, â–imp, â–inc, â–inf, â–ins, â–jam, â–jou, â–jus, â–les, â–lui, â–mad, â–mal, â–man, â–mar, â–mer, â–mes, â–mil, â–min, â–mis, â–moi, â–mom, â–mon, ...]  \n",
       "7                                                                                                                                                                                    [able, ages, agne, aine, ains, aire, aiss, ales, alis, ance, ande, ange, anis, ante, ants, arde, asse, atre, cher, ches, coup, dent, duit, elle, ence, ends, enir, enne, ense, ente, ents, eure, eurs, euse, iens, ille, ingt, ions, ique, iste, iter, itÃ©s, iÃ¨me, iÃ¨re, jour, lier, lles, lopp, mble, ment, mple, nent, oins, oire, olog, onne, onom, orte, orti, oses, otre, ours, ouve, prÃ¨s, puis, quer, ques, quoi, rais, rait, rent, ress, sion, tant, tion, tout, tres, ttre, uite, ures, veau, vent, vers, voir, vous, Ã¨res, Ãªtes, Ãªtre]  \n",
       "8   [â–alle, â–appr, â–arri, â–avec, â–avez, â–beau, â–bien, â–cela, â–cent, â–cert, â–ceux, â–char, â–cher, â–chez, â–cinq, â–comb, â–comm, â–comp, â–conc, â–conf, â–conn, â–cons, â–cont, â–coup, â–cour, â–cÃ´tÃ©, â–dans, â–dern, â–deux, â–dieu, â–diff, â–dire, â–doit, â–donc, â–donn, â–dont, â–dÃ©jÃ , â–effe, â–elle, â–ense, â–fais, â–fait, â–faut, â–fois, â–fond, â–form, â–fran, â–gens, â–gros, â–hein, â–hist, â–indi, â–intÃ©, â–jour, â–leur, â–long, â–lors, â–main, â–mais, â–mois, â–mont, â–mort, â–mÃªme, â–neuf, â–nous, â–parf, â–part, â–pass, â–pays, â–pens, â–pers, â–peti, â–peut, â–peux, â–plan, â–plus, â–plut, â–poin, â–poli, â–pour, â–pouv, â–pres, â–prin, â–pris, â–prob, â–prof, â–prop, â–prÃ©s, â–puis, â–quar, â–quel, â–ques, â–quoi, â–rais, â–rapp, â–repr, â–retr, â–rien, â–sain, â–sais, ...]  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [ables, aient, aines, aires, alitÃ©, ances, anger, ation, ature, ction, endre, eures, ieurs, ilitÃ©, iques, jourd, jours, lique, ments, ouver, ouvez, sible, sieur, tions, tique]  \n",
       "10                                                                                                                                                                                           [â–ainsi, â–aller, â–allez, â–alors, â–aprÃ¨s, â–assez, â–aussi, â–autre, â–avais, â–avait, â–avant, â–avoir, â–avons, â–bonne, â–celui, â–cette, â–chose, â–cinqu, â–comme, â–compr, â–contr, â–crois, â–droit, â–elles, â–enfin, â–entre, â–europ, â–faire, â–franÃ§, â–grand, â–heure, â–homme, â–inter, â–jours, â–jusqu, â–juste, â–leurs, â–mieux, â–mille, â–milli, â–moins, â–monde, â–notre, â–nouve, â–ouais, â–parce, â–parle, â–parti, â–passe, â–pense, â–perme, â–petit, â–place, â–point, â–premi, â–quand, â–temps, â–toute, â–trans, â–trois, â–vidÃ©o, â–vingt, â–voilÃ , â–votre, â–Ã©tait]  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [amment, ations, atique, endant, lement, lleurs, nement, sition, tement, velopp, vement]  \n",
       "12                                                                                                                                                                                                                                                                                                                                                  [â–<UNK>â–, â–accord, â–annÃ©es, â–autres, â–besoin, â–chaque, â–choses, â–commen, â–commun, â–compte, â–contin, â–contre, â–demand, â–depuis, â–diffÃ©r, â–donner, â–encore, â–europÃ©, â–france, â–grande, â–import, â–jamais, â–mettre, â–moment, â–niveau, â–nombre, â–parler, â–partie, â–passer, â–petite, â–pouvez, â–quatre, â–regard, â–rÃ©pond, â–savoir, â–simple, â–toutes, â–travai, â–trente, â–trouve, â–utilis]  \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [alement, llement]  \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [â–appelle, â–aujourd, â–comment, â–enfants, â–ensuite, â–entrepr, â–exemple, â–mainten, â–pendant, â–personn, â–pouvoir, â–premier, â–prendre, â–prÃ©sent, â–quelque, â–souvent, â–surtout, â–travail, â–Ã©taient]  \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [â–ailleurs, â–beaucoup, â–fonction, â–franÃ§ais, â–histoire, â–monsieur, â–personne, â–pourquoi, â–premiÃ¨re, â–quelques, â–question, â–soixante, â–toujours, â–vraiment]  \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [â–cinquante, â–important, â–personnes, â–plusieurs, â–Ã©galement]  \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [â–maintenant]  \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vocabulary(\"Perruche\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â˜ª Understand how tokenizers work with Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>size</th>\n",
       "      <th>#tokens</th>\n",
       "      <th>fert.</th>\n",
       "      <th>encoded tokens</th>\n",
       "      <th>round-trip</th>\n",
       "      <th>decoded text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jais</td>\n",
       "      <td>64k</td>\n",
       "      <td>13</td>\n",
       "      <td>3.2</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€â€ï¿½â”ƒâ€â€ï¿½â”ƒâ€â”ƒØŒâ”ƒÙƒÙŠÙâ”ƒØ­â”ƒØ§Ù„Ùƒâ€Pierreâ”ƒâ€â€-â”ƒâ€â€Jeanâ”ƒâ€â”ƒÙ…Ø±Ø­Ø¨Ø§â”ƒÙ‹â€&lt;BOS&gt;</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>â€&lt;EOS&gt;â€ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸâ€Jean-Pierre â€ Ù…Ø±Ø­Ø¨Ø§Ù‹ â€&lt;BOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aya</td>\n",
       "      <td>255k</td>\n",
       "      <td>15</td>\n",
       "      <td>3.8</td>\n",
       "      <td>â€&lt;EOS&gt;â”ƒâ€Ø­Ø§Ù„â”ƒÙƒâ”ƒØŸâ€â–â”ƒâ€Ùƒâ”ƒÙŠÙâ€â–â”ƒâ€â”ƒØŒâ€Pierreâ”ƒâ€â€-â”ƒâ€â€â–Jeanâ”ƒâ€â”ƒÙ…Ø±â”ƒØ­â”ƒØ¨Ø§â”ƒÙ‹â€&lt;BOS&gt;</td>\n",
       "      <td>âœ…</td>\n",
       "      <td>â€&lt;EOS&gt;â€ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸâ€Jean-Pierre â€Ù…Ø±Ø­Ø¨Ø§Ù‹ â€&lt;BOS&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tokenizer size   #tokens  fert.  \\\n",
       "0  Jais       64k  13       3.2     \n",
       "1   Aya      255k  15       3.8     \n",
       "\n",
       "  encoded tokens                                                       \\\n",
       "0      â€<EOS>â”ƒâ€â€ï¿½â”ƒâ€â€ï¿½â”ƒâ€â”ƒØŒâ”ƒÙƒÙŠÙâ”ƒØ­â”ƒØ§Ù„Ùƒâ€Pierreâ”ƒâ€â€-â”ƒâ€â€Jeanâ”ƒâ€â”ƒÙ…Ø±Ø­Ø¨Ø§â”ƒÙ‹â€<BOS>   \n",
       "1  â€<EOS>â”ƒâ€Ø­Ø§Ù„â”ƒÙƒâ”ƒØŸâ€â–â”ƒâ€Ùƒâ”ƒÙŠÙâ€â–â”ƒâ€â”ƒØŒâ€Pierreâ”ƒâ€â€-â”ƒâ€â€â–Jeanâ”ƒâ€â”ƒÙ…Ø±â”ƒØ­â”ƒØ¨Ø§â”ƒÙ‹â€<BOS>   \n",
       "\n",
       "  round-trip decoded text                                     \n",
       "0  âœ…          â€<EOS>â€ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸâ€Jean-Pierre â€ Ù…Ø±Ø­Ø¨Ø§Ù‹ â€<BOS>  \n",
       "1  âœ…           â€<EOS>â€ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸâ€Jean-Pierre â€Ù…Ø±Ø­Ø¨Ø§Ù‹ â€<BOS>  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"Ù…Ø±Ø­Ø¨Ø§Ù‹ Jean-PierreØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\"\n",
    "\n",
    "test_tokenizers_on_text(input, [\"Jais\", \"Aya\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
